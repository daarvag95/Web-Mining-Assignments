{"title": "What is an intuitive explanation of the Expectation Maximization technique? [closed]", "question": {"text": "\n\n\n\nExpectation Maximization (EM) is a kind of probabilistic method to classify data. Please correct me if I am wrong if it is not a classifier. \nWhat is an intuitive explanation of this EM technique? What is  here and what is being ?\n", "comments": [", Nature Biotechnology 26, 897â€“899 (2008) has a nice picture that illustrates how the algorithm works.", "@chl In part b of the , how did they get the values of the probability distribution on the Z (i.e., 0.45xA, 0.55xB, etc.)?", "You can look at this question ", " to the picture that @chl mentioned."]}, "answers": [{"text": "\nNote: the code behind this answer can be found .\n\nSuppose we have some data sampled from two different groups, red and blue:\n\nHere, we can see which data point belongs to the red or blue group. This makes it easy to find the parameters that characterise each group. For example, the mean of the red group is around 3, the mean of the blue group is around 7 (and we could find the exact means if we wanted).\nThis is, generally speaking, known as maximum likelihood estimation. Given some data, we compute the value of a parameter (or parameters) that best explains that data.\nNow imagine that we cannot see which value was sampled from which group. Everything looks purple to us:\n\nHere we have the knowledge that there are two groups of values, but we don't know which group any particular value belongs to. \nCan we still estimate the means for the red group and blue group that best fit this data?\nYes, often we can! Expectation Maximisation gives us a way to do it. The very general idea behind the algorithm is this:\n\nStart with an initial estimate of what each parameter might be.\nCompute the likelihood that each parameter produces the data point.\nCalculate weights for each data point indicating whether it is more red or more blue based on the likelihood of it being produced by a parameter. Combine the weights with the data (expectation).\nCompute a better estimate for the parameters using the weight-adjusted data (maximisation).\nRepeat steps 2 to 4 until the parameter estimate converges (the process stops producing a different estimate).\n\nThese steps need some further explanation, so I'll walk through the  problem described above.\nExample: estimating mean and standard deviation\nI'll use Python in this example, but the code should be fairly easy to understand if you're not familiar with this language. \nSuppose we have two groups, red and blue, with the values distributed as in the image above. Specifically, each group contains a value drawn from a  with the following parameters:\n\nHere is an image of these red and blue groups again (to save you from having to scroll up):\n\nWhen we can see the colour of each point (i.e. which group it belongs to), it's very easy to estimate the mean and standard deviation for each each group. We just pass the red and blue values to the builtin functions in NumPy. For example:\n\nBut what if we can't see the colours of the points? That is, instead of red or blue, every point has been coloured purple.\nTo try and recover the mean and standard deviation parameters for the red and blue groups, we can use Expectation Maximisation.\nOur first step (step 1 above) is to guess at the parameter values for each group's mean and standard deviation. We don't have to guess intelligently; we can pick any numbers we like:\n\nThese parameter estimates produce bell curves that look like this:\n\nThese are bad estimates. Both means (the vertical dotted lines) look far off any kind of \"middle\" for sensible groups of points, for instance. We want to improve these estimates.\nThe next step (step 2) is to compute the likelihood of each data point appearing under the current parameter guesses:\n\nHere, we have simply put each data point into the  for a normal distribution using our current guesses at the mean and standard deviation for red and blue. This tells us, for example, that with our current guesses the data point at 1.761 is much more likely to be red (0.189) than blue (0.00003). \nFor each data point, we can turn these two likelihood values into weights (step 3) so that they sum to 1 as follows:\n\nWith our current estimates and our newly-computed weights, we can now compute new estimates for the mean and standard deviation of the red and blue groups (step 4).\nWe twice compute the mean and standard deviation using all data points, but with the different weightings: once for the red weights and once for the blue weights.\nThe key bit of intuition is that the greater the weight of a colour on a data point, the more the data point influences the next estimates for that colour's parameters. This has the effect of \"pulling\" the parameters in the right direction.\n\nWe have new estimates for the parameters. To improve them again, we can jump back to step 2 and repeat the process. We do this until the estimates converge, or after some number of iterations have been performed (step 5).\nFor our data, the first five iterations of this process look like this (recent iterations have stronger appearance):\n\nWe see that the means are already converging on some values, and the shapes of the curves (governed by the standard deviation) are also becoming more stable. \nIf we continue for 20 iterations, we end up with the following:\n\nThe EM process has converged to the following values, which turn out to very close to the actual values (where we can see the colours - no hidden variables):\n\nIn the code above you may have noticed that the new estimation for standard deviation was computed using the previous iteration's estimate for the mean. Ultimately it does not matter if we compute a new value for the mean first as we are just finding the (weighted) variance of values around some central point. We will still see the estimates for the parameters converge.\n", "comments": ["what if we even dont know the number of normal distributions from which this is coming from? Here you have taken an example of k=2 distributions, can we also estimate k, and the k parameter sets?", "@stackit: I'm not sure there's a straightforward general way to compute the most likely value of k as part of the EM process in this case. The main issue is that we would need to start EM with estimates for each of parameters we want to find, and that entails that we need to know/estimate k before we begin. It is possible, however, to estimate the proportion of points belonging to a group via EM here. Maybe if we overestimate k, the proportion of all but two of the groups would drop to near zero. I haven't experimented with this, so I don't know how well it would work in practice.", "@AlexRiley Can you say a bit more about the formulas for computing the new mean and standard deviation estimates?", "@AlexRiley Thanks for the explanation. Why are the new standard deviation estimates calculated using the old guess of the mean? What if the new estimates of the mean are found first?", "@Lemon GoodDeeds Kaushal - apologies for my late reply to your questions. I've tried to edit the answer to address the points you've raised. I have also made all of the code used in this answer accessible in a notebook   (which also includes more details explanations of some points I touched upon)."]}, {"text": "\nEM is an algorithm for maximizing a likelihood function when some of the variables in your model are unobserved (i.e. when you have latent variables).  \nYou might fairly ask, if we're just trying to maximize a function, why don't we just use the existing machinery for maximizing a function.  Well, if you try to maximize this by taking derivatives and setting them to zero, you find that in many cases the first-order conditions don't have a solution.  There's a chicken-and-egg problem in that to solve for your model parameters you need to know the distribution of your unobserved data; but the distribution of your unobserved data is a function of your model parameters.  \nE-M tries to get around this by iteratively guessing a distribution for the unobserved data, then estimating the model parameters by maximizing something that is a lower bound on the actual likelihood function, and repeating until convergence:\nThe EM algorithm \nStart with guess for values of your model parameters\nE-step:  For each datapoint that has missing values, use your model equation to solve for the distribution of the missing data given your current guess of the model parameters and given the observed data (note that you are solving for a distribution for each missing value, not for the expected value).  Now that we have a distribution for each missing value, we can calculate the expectation of the likelihood function with respect to the unobserved variables.  If our guess for the model parameter was correct, this expected likelihood will be the actual likelihood of our observed data; if the parameters were not correct, it will just be a lower bound.\nM-step:  Now that we've got an expected likelihood function with no unobserved variables in it, maximize the function as you would in the fully observed case, to get a new estimate of your model parameters.\nRepeat until convergence.\n", "comments": ["I do not understand your E-step. Part of the problem is that as I am learning this stuff, I can't find people who use the same terminology. So what do you mean by model equation? I don't know what you mean by solving for a probability distribution?"]}, {"text": "\nHere is a straight-forward recipe to understand the Expectation Maximisation algorithm:\n1- Read this  by Do and Batzoglou.\n2- You may have question marks in your head, have a look at the explanations on this maths stack exchange .\n3- Look at this code that I wrote in Python that explains the example in the EM tutorial paper of item 1:\nWarning : The code may be messy/suboptimal, since I am not a Python developer. But it does the job.\n\n", "comments": ["I find that your program will result in both A and B to 0.66, I also implement it using scala, also find that the result is 0.66, can you help check that ?", "Using a spreadsheet, I only find your 0.66 results if my initial guesses are equal. Otherwise, I can reproduce the output of the tutorial.", "@zjffdu, how many iterations does the EM run before returning you 0.66? If you initialise with equal values it may be getting stuck at a local maximum and you will see that the number of iterations is extremely low (since there is no improvement).", "You can also check out this  by Andrew Ng and "]}, {"text": "\nTechnically the term \"EM\" is a bit underspecified, but I assume you refer to the Gaussian Mixture Modelling cluster analysis technique, that is an instance of the general EM principle.\nActually, EM cluster analysis is not a classifier. I know that some people consider clustering to be \"unsupervised classification\", but actually cluster analysis is something quite different.\nThe key difference, and the big misunderstanding classification people always have with cluster analysis is that: in cluster analaysis, there is no \"correct solution\". It is a knowledge discovery method, it is actually meant to find something new! This makes evaluation very tricky. It is often evaluated using a known classification as reference, but that is not always appropriate: the classification you have may or may not reflect what is in the data.\nLet me give you an example: you have a large data set of customers, including gender data. A method that splits this data set into \"male\" and \"female\" is optimal when you compare it with the existing classes. In a \"prediction\" way of thinking this is good, as for new users you could now predict their gender. In a \"knowledge discovery\" way of thinking this is actually bad, because you wanted to discover some new structure in the data. A method that would e.g. split the data into elderly people and kids however would score as worse as it can get with respect to the male/female class. However, that would be an excellent clustering result (if the age wasn't given).\nNow back to EM. Essentially it assumes that your data is composed of multiple multivariate normal distributions (note that this is a very strong assumption, in particular when you fix the number of clusters!). It then tries to find a local optimal model for this by alternatingly improving the model and the object assignment to the model.\nFor best results in a classification context, choose the number of clusters larger than the number of classes, or even apply the clustering to single classes only (to find out whether there is some structure within the class!).\nSay you want to train a classifier to tell apart \"cars\", \"bikes\" and \"trucks\". There is little use in assuming the data to consist of exactly 3 normal distributions. However, you may assume that there is more than one type of cars (and trucks and bikes). So instead of training a classifier for these three classes, you cluster cars, trucks and bikes into 10 clusters each (or maybe 10 cars, 3 trucks and 3 bikes, whatever), then train a classifier to tell apart these 30 classes, and then merge the class result back to the original classes. You may also discover that there is one cluster that is particularly hard to classify, for example Trikes. They're somewhat cars, and somewhat bikes. Or delivery trucks, that are more like oversized cars than trucks.\n", "comments": ["how is EM underspecified?", "There is more than one version of it. Technically, you can call Lloyd style k-means \"EM\", too. You need to specify what model you use."]}, {"text": "\nOther answers being good, i will try to provide another perspective and tackle the intuitive part of the question.\n is a variant of a class of iterative algorithms using \nExcerpt (emphasis mine):\n\nIn mathematics, a duality, generally speaking, translates concepts,\n  theorems or mathematical structures into other concepts, theorems or\n  structures, in a one-to-one fashion, often (but not always) by means\n  of an involution operation: if the dual of A is B, then the dual of B\n  is A. Such involutions sometimes have fixed points, so that the dual\n  of A is A itself\n\nUsually a dual B of an object A is related to A in some way that preserves some symmetry or compatibility. For example AB = const\nExamples of iterative algorithms, employing duality (in the previous sense) are:\n\n\n\n\n (see also )\n(.. other similar algorithms..)\n\nIn a similar fashion, :\n\n..[EM] is seen as maximizing a joint function of the parameters and of\n  the distribution over the unobserved variables.. The E-step maximizes\n  this function with respect to the distribution over the unobserved\n  variables; the M-step with respect to the parameters..\n\nIn an iterative algorithm using duality there is the explicit (or implicit) assumption of an equilibrium (or fixed) point of convergence (for EM this is proved using Jensen's inequality)\nSo the outline of such algorithms is:\n\nE-like step: Find best solution x with respect to given y being held constant.\nM-like step (dual): Find best solution y with respect to x (as computed in previous step) being held constant.\nCriterion of Termination/Convergence step: Repeat steps 1, 2 with the updated values of x,y until convergence (or specified number of iterations is reached)\n\nNote that when such an algorithm converges to a (global) optimum, it has found a configuration which is best in both senses (i.e in both the x domain/parameters and the y domain/parameters). However the algorithm can just find a local optimum and not the global optimum.\ni would say this is the intuitive description of the outline of the algorithm\nFor the statistical arguments and applications, other answers have given good explanations (check also references in this answer)\n", "comments": []}, {"text": "\nThe accepted answer references the , which does a decent job explaining EM. There is also a  that explains the paper in more detail. \nTo recap, here is the scenario:\n\nIn the case of the first trial's question, intuitively we'd think B generated it since the proportion of heads matches B's bias very well... but that value was just a guess, so we can't be sure.  \nWith that in mind, I like to think of the EM solution like this:\n\nEach trial of flips gets to 'vote' on which coin it likes the most\n\n\nThis is based on how well each coin fits its distribution  \nOR, from the point of view of the coin, there is high expectation of seeing this trial relative to the other coin (based on log likelihoods).\n\nDepending on how much each trial likes each coin, it can update the guess of that coin's parameter (bias).  \n\n\nThe more a trial likes a coin, the more it gets to update the coin's bias to reflect its own!  \nEssentially the coin's biases are updated by combining these weighted updates across all trials, a process called (maximazation), which refers to trying to get the best guesses for each coin's bias given a set of trials. \n\n\nThis may be an oversimplification (or even fundamentally wrong on some levels), but I hope this helps on an intuitive level!\n", "comments": []}, {"text": "\nEM is used to maximize the likelihood of a model Q with latent variables Z.\nIt's an iterative optimization.\n\ne-step:\ngiven current estimation of Z calculate the expected loglikelihood function \nm-step:\nfind theta which maximizes this Q\nGMM Example:\ne-step: estimate label assignments for each datapoint given the current gmm-parameter estimation\nm-step: maximize a new theta given the new label assigments\nK-means is also an EM algorithm and there is a lot of explaining animations on K-means.\n", "comments": []}, {"text": "\nUsing the same article by Do and Batzoglou cited in Zhubarb's answer, I implemented EM for that problem in Java. The comments to his answer show that the algorithm gets stuck at a local optimum, which also occurs with my implementation if the parameters thetaA and thetaB are the same.\nBelow is the standard output of my code, showing the convergence of the parameters.\n\nBelow is my Java implementation of EM to solve the problem in (Do and Batzoglou, 2008). The core part of the implementation is the loop to run EM until the parameters converge.\n\nBelow is the entire code.\n\n", "comments": []}]}