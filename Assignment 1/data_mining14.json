{"title": "R Random Forests Variable Importance", "question": {"text": "\nI am trying to use the random forests package for classification in R.\nThe Variable Importance Measures listed are:\n\nmean raw importance score of variable x for class 0\nmean raw importance score of variable x for class 1\n\n\n\nNow I know what these \"mean\" as in I know their definitions.  What I want to know is how to use them.\nWhat I really want to know is what these values mean in only the context of how accurate they are, what is a good value, what is a bad value, what are the maximums and minimums, etc.\nIf a variable has a high  or  does that mean it is important or unimportant?  Also any information on raw scores could be useful too.\nI want to know everything there is to know about these numbers that is relevant to the application of them.  \nAn explanation that uses the words 'error', 'summation', or 'permutated' would be less helpful then a simpler explanation that didn't involve any discussion of how random forests works.\nLike if I wanted someone to explain to me how to use a radio, I wouldn't expect the explanation to involve how a radio converts radio waves into sound.\n", "comments": []}, "answers": [{"text": "\n\nAn explanation that uses the words 'error', 'summation', or 'permutated'\n  would be less helpful then a simpler explanation that didn't involve any\n  discussion of how random forests works.\nLike if I wanted someone to explain to me how to use a radio, I wouldn't\n  expect the explanation to involve how a radio converts radio waves into sound.\n\nHow would you explain what the numbers in WKRP 100.5 FM \"mean\" without going into the pesky technical details of wave frequencies?  Frankly parameters and related performance issues with Random Forests are difficult to get your head around even if you understand some technical terms.\nHere's my shot at some answers:\n\n-mean raw importance score of variable x for class 0\n-mean raw importance score of variable x for class 1\n\nSimplifying from the Random Forest , raw importance score measures how much more helpful than random a particular predictor variable is in successfully classifying data.\n\n-MeanDecreaseAccuracy\n\nI think this is only in the , and I believe it measures how much inclusion of this predictor in the model reduces classification error.\n\n-MeanDecreaseGini\n\n is defined as \"inequity\" when used in describing a society's distribution of income, or a measure of \"node impurity\" in tree-based classification.  A low Gini (i.e. higher descrease in Gini) means that a particular predictor variable plays a greater role in partitioning the data into the defined classes.  It's a hard one to describe without talking about the fact that data in classification trees are split at individual nodes based on values of predictors.  I'm not so clear on how this translates into better performance.\n", "comments": ["Please, include the link to the Gini definition that is actually used for node splitting: "]}, {"text": "\nFor your immediate concern: higher values mean the variables are more important.  This should be true for all the measures you mention.\nRandom forests give you pretty complex models, so it can be tricky to interpret the importance measures.  If you want to easily understand what your variables are doing, don't use RFs.  Use linear models or a (non-ensemble) decision tree instead.\nYou said:\n\nAn explanation that uses the words\n  'error', 'summation', or 'permutated'\n  would be less helpful then a simpler\n  explanation that didn't involve any\n  discussion of how random forests\n  works.\n\nIt's going to be awfully tough to explain much more than the above unless you dig in and learn what about random forests.  I assume you're complaining about either the manual, or the section from Breiman's manual:\n\nTo figure out how important a variable is, they fill it with random junk (\"permute\" it), then see how much predictive accuracy decreases.  MeanDecreaseAccuracy and MeanDecreaseGini work this way.  I'm not sure what the raw importance scores are.\n", "comments": []}, {"text": "\nInterpretability is kinda tough with Random Forests. While RF is an extremely robust classifier it makes its predictions democratically. By this I mean you build hundreds or thousands of trees by taking a random subset of your variables and a random subset of your data and build a tree. Then make a prediction for all the non-selected data and save the prediction. Its robust because it deals well with the vagaries of your data set, (ie it smooths over randomly high/low values, fortuitous plots/samples, measuring the same thing 4 different ways, etc). However if you have some highly correlated variables, both may seem important as they are not both always included in each model. \nOne potential approach with random forests may be to help whittle down your predictors then switch to regular CART or try the PARTY package for inference based tree models. However then you must be wary about data mining issues, and making inferences about parameters.\n", "comments": []}]}