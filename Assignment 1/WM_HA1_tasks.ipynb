{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web-Mining - Assignment 1 (25 points total)\n",
    "\n",
    "This **Home Assignment** is to be submitted and you will be given points for each of the tasks. It familiarizes you with basics of *web crawling* and standard text preprocessing.\n",
    "\n",
    "## Formalities\n",
    "**Submit in a group of 3-4 people until 11.05.2020 23:59CET. The deadline is strict!**\n",
    "\n",
    "## Evaluation and Grading\n",
    "General advice for programming excercises at *CSSH*:\n",
    "Evaluation of your submission is done semi-automatically. Think of it as this notebook being \n",
    "executed once. Afterwards, some test functions are appended to this file and executed respectively.\n",
    "\n",
    "Therefore:\n",
    "* Submit valid _Python3_ code only!\n",
    "* Use external libraries only when specified by task.\n",
    "* Ensure your definitions (functions, classes, methods, variables) follow the specification if\n",
    "  given. The concrete signature of e.g. a function usually can be inferred from task description, \n",
    "  code skeletons and test cases.\n",
    "* Ensure the notebook does not rely on current notebook or system state!\n",
    "  * Use `Kernel --> Restart & Run All` to see if you are using any definitions, variables etc. that \n",
    "    are not in scope anymore.\n",
    "  * Double check if your code relies on presence of files or directories other than those mentioned\n",
    "    in given tasks. Tests run under Linux, hence don't use Windows style paths \n",
    "    (`some\\path`, `C:\\another\\path`). Also, use paths only that are relative to and within your\n",
    "    working directory (OK: `some/path`, `./some/path`; NOT OK: `/home/alice/python`, \n",
    "    `../../python`).\n",
    "* Keep your code idempotent! Running it or parts of it multiple times must not yield different\n",
    "  results. Minimize usage of global variables.\n",
    "* Ensure your code / notebook terminates in reasonable time.\n",
    "\n",
    "**There's a story behind each of these points! Don't expect us to fix your stuff!**\n",
    "\n",
    "Regarding the scores, you will get no points for a task if:\n",
    "- your function throws an unexpected error (e.g. takes the wrong number of arguments)\n",
    "- gets stuck in an infinite loop\n",
    "- takes much much longer than expected (e.g. >1s to compute the mean of two numbers)\n",
    "- does not produce the desired output (e.g. returns an descendingly sorted list even though we asked for ascending, returns the mean and the std even though we asked for only the mean, prints an output instead of returning it, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credentials of all team members (you may add or remove items from the dictionary)\n",
    "team_members = [\n",
    "    {\n",
    "        'first_name': 'Haron',\n",
    "        'last_name': 'Nqiri',\n",
    "        'student_id': 343289\n",
    "    },\n",
    "    {\n",
    "        'first_name': 'Mike',\n",
    "        'last_name': 'GrÃ¼ne',\n",
    "        'student_id': 381076\n",
    "    },\n",
    "    {\n",
    "        'first_name': 'Seyed Pouria',\n",
    "        'last_name': 'Mirelmi',\n",
    "        'student_id': 416910\n",
    "    },\n",
    "    {\n",
    "        'first_name': 'Pruthvi',\n",
    "        'last_name': 'Hegde',\n",
    "        'student_id': 404809\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Crawling web pages (total of 15 points)\n",
    "We are interested in obtaining questions and their answers on the `stackoverflow` platform. Use the `BeautifulSoup` and `requests` packages for this task.\n",
    "\n",
    "## a) Simple spider (5)\n",
    "\n",
    "Write a function `get_questions` that takes a tag (like `\"data-mining\"`) and a number `n` and returns a list containing the hyperlinks (as strings) to the top n questions by votes for the given tag. Assume the tag exists.\n",
    "\n",
    "The first link for \"data-mining\" is: https://stackoverflow.com/questions/12146914/what-is-the-difference-between-linear-regression-and-logistic-regression \n",
    "\n",
    "Notice that `n` might exceed the number of questions appearing on the first page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_questions(tag : str, n : int, pause=0.5) -> List[str]:\n",
    "    ret = []\n",
    "    pageIndex = 1\n",
    "    while len(ret) < n:\n",
    "        page = requests.get('https://stackoverflow.com/questions/tagged/' + tag + '?tab=votes&page=' + str(pageIndex))\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        for question in soup.find_all('div', class_=\"t-\" + tag, limit=(n - len(ret))):\n",
    "            ret.append('https://stackoverflow.com' + question.parent.find('a', class_='question-hyperlink').get('href'))\n",
    "        pageIndex += 1\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://stackoverflow.com/questions/2480650/what-is-the-role-of-the-bias-in-neural-networks\n",
      "https://stackoverflow.com/questions/879432/what-is-the-difference-between-a-generative-and-a-discriminative-algorithm\n",
      "https://stackoverflow.com/questions/33759623/tensorflow-how-to-save-restore-a-model\n",
      "https://stackoverflow.com/questions/10059594/a-simple-explanation-of-naive-bayes-classification\n",
      "https://stackoverflow.com/questions/307291/how-does-the-google-did-you-mean-algorithm-work\n",
      "https://stackoverflow.com/questions/4752626/epoch-vs-iteration-when-training-neural-networks\n",
      "https://stackoverflow.com/questions/34240703/what-are-logits-what-is-the-difference-between-softmax-and-softmax-cross-entrop\n",
      "https://stackoverflow.com/questions/11632516/what-are-advantages-of-artificial-neural-networks-over-support-vector-machines\n",
      "https://stackoverflow.com/questions/41455101/what-is-the-meaning-of-the-word-logits-in-tensorflow\n",
      "https://stackoverflow.com/questions/1832076/what-is-the-difference-between-supervised-learning-and-unsupervised-learning\n",
      "https://stackoverflow.com/questions/29831489/convert-array-of-indices-to-1-hot-encoded-numpy-array\n",
      "https://stackoverflow.com/questions/12146914/what-is-the-difference-between-linear-regression-and-logistic-regression\n",
      "https://stackoverflow.com/questions/34968722/how-to-implement-the-softmax-function-in-python\n",
      "https://stackoverflow.com/questions/34518656/how-to-interpret-loss-and-accuracy-for-a-machine-learning-model\n",
      "https://stackoverflow.com/questions/13610074/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validatio\n",
      "https://stackoverflow.com/questions/2595176/which-machine-learning-classifier-to-choose-in-general\n",
      "https://stackoverflow.com/questions/10592605/save-classifier-to-disk-in-scikit-learn\n",
      "https://stackoverflow.com/questions/598726/overwhelmed-by-machine-learning-is-there-an-ml101-book\n",
      "https://stackoverflow.com/questions/5064928/difference-between-classification-and-clustering-in-data-mining\n",
      "https://stackoverflow.com/questions/42081257/why-binary-crossentropy-and-categorical-crossentropy-give-different-performances\n"
     ]
    }
   ],
   "source": [
    "# example usage\n",
    "result = get_questions(\"machine-learning\", 20)\n",
    "for link in result:\n",
    "    print(link)\n",
    "# expected output\n",
    "# https://stackoverflow.com/questions/2480650/what-is-the-role-of-the-bias-in-neural-networks\n",
    "# https://stackoverflow.com/questions/879432/what-is-the-difference-between-a-generative-and-a-discriminative-algorithm\n",
    "# https://stackoverflow.com/questions/33759623/tensorflow-how-to-save-restore-a-model\n",
    "# https://stackoverflow.com/questions/10059594/a-simple-explanation-of-naive-bayes-classification\n",
    "# https://stackoverflow.com/questions/307291/how-does-the-google-did-you-mean-algorithm-work\n",
    "# https://stackoverflow.com/questions/4752626/epoch-vs-iteration-when-training-neural-networks\n",
    "# https://stackoverflow.com/questions/34240703/what-is-logits-what-is-the-difference-between-softmax-and-softmax-cross-entropy\n",
    "# https://stackoverflow.com/questions/11632516/what-are-advantages-of-artificial-neural-networks-over-support-vector-machines\n",
    "# https://stackoverflow.com/questions/41455101/what-is-the-meaning-of-the-word-logits-in-tensorflow\n",
    "# https://stackoverflow.com/questions/1832076/what-is-the-difference-between-supervised-learning-and-unsupervised-learning\n",
    "# https://stackoverflow.com/questions/12146914/what-is-the-difference-between-linear-regression-and-logistic-regression\n",
    "# https://stackoverflow.com/questions/29831489/convert-array-of-indices-to-1-hot-encoded-numpy-array\n",
    "# https://stackoverflow.com/questions/34968722/how-to-implement-the-softmax-function-in-python\n",
    "# https://stackoverflow.com/questions/34518656/how-to-interpret-loss-and-accuracy-for-a-machine-learning-model\n",
    "# https://stackoverflow.com/questions/13610074/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validatio\n",
    "# https://stackoverflow.com/questions/2595176/which-machine-learning-classifier-to-choose-in-general\n",
    "# https://stackoverflow.com/questions/10592605/save-classifier-to-disk-in-scikit-learn\n",
    "# https://stackoverflow.com/questions/598726/overwhelmed-by-machine-learning-is-there-an-ml101-book\n",
    "# https://stackoverflow.com/questions/5064928/difference-between-classification-and-clustering-in-data-mining\n",
    "# https://stackoverflow.com/questions/42081257/why-binary-crossentropy-and-categorical-crossentropy-give-different-performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do', 'https://stackoverflow.com/questions/419163/what-does-if-name-main-do', 'https://stackoverflow.com/questions/394809/does-python-have-a-ternary-conditional-operator']\n"
     ]
    }
   ],
   "source": [
    "# example usage\n",
    "print(get_questions(\"python\", 3))\n",
    "#['https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do',\n",
    "# 'https://stackoverflow.com/questions/419163/what-does-if-name-main-do',\n",
    "# 'https://stackoverflow.com/questions/394809/does-python-have-a-ternary-conditional-operator']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Processing a page (8)\n",
    "\n",
    "Write a function `parse_question` that takes a hyperlink, as a string, to a StackOverflow questions and returns a dictionary containing the text that is produced by the users. This includes the question (+ comments) and the answers (+ comments).\n",
    "\n",
    "We are not interested in other content like: images, `<aside>` sections, html tags, user_information, `<code>` sections and `<a>` links.\n",
    "We are also not interested in information on edits, dates etc. As that text is not user generated we also disregard all functional text-content like share, edit, follow, flag, add comment (the kind of button things). \n",
    "\n",
    "! Ignore everything that is not inside the div with `id=\"mainbar\"` !\n",
    "\n",
    "Use BeautifulSoup to process the HTML of the page. There you can get the text of an element through `.text`. You can remove elements by calling `.decompose()` on them.\n",
    "\n",
    "The structure of the result is:\n",
    "\n",
    "```python\n",
    "{'title': 'The title',\n",
    " 'question': {'text' : 'How to learn web-mining?',\n",
    "              'comments':['Good question', 'Sounds\\n interesting']},\n",
    " 'answers' : [{'text':'Do a course at CSSH!', \n",
    "               'comments' : ['You will learn a lot', 'Good stuff']},\n",
    "              {'text':'Learn on youtube', 'comments' : []}, ]}\n",
    "```\n",
    "You can also find an example of the  processed page https://stackoverflow.com/questions/12146914/what-is-the-difference-between-linear-regression-and-logistic-regression on moodle.\n",
    "\n",
    "\n",
    "Notice, that the answers might be spread across multiple pages, see:\n",
    "https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts the given object to text after removing unwanted parts\n",
    "def convert_to_text(te: BeautifulSoup):\n",
    "    for unTag in te.find_all(['code', 'img', 'aside', 'a']):\n",
    "        unTag.decompose()\n",
    "    return te.text\n",
    "\n",
    "# returns the title of the stackoverflow question\n",
    "def get_question_title(soup: BeautifulSoup):\n",
    "    questionHeader = soup.find('div',id='question-header')\n",
    "    return questionHeader.find('a', class_='question-hyperlink').text\n",
    "\n",
    "# returns the text of the question or an answer\n",
    "def get_main_text(post: BeautifulSoup):\n",
    "    postDiv = post.find('div', class_=['postcell','answercell'])\n",
    "    textDiv = postDiv.find('div', class_='js-post-body')\n",
    "    return convert_to_text(textDiv)\n",
    "\n",
    "# returns a list of comments on the current page\n",
    "def get_comments(post: BeautifulSoup):\n",
    "    comments = post.find_all('span', class_='comment-copy')\n",
    "    retComments = []\n",
    "    for comment in comments:\n",
    "        retComments.append(convert_to_text(comment))\n",
    "    return retComments\n",
    "\n",
    "# returns a question or answer object\n",
    "def get_post(pDiv: BeautifulSoup):\n",
    "    ret = {}\n",
    "    ret['text'] = get_main_text(pDiv)\n",
    "    ret['comments'] = get_comments(pDiv)\n",
    "    return ret\n",
    "\n",
    "# returns a list of all answers objects on the current site\n",
    "def get_answers(answersDiv: BeautifulSoup):\n",
    "    ret = []\n",
    "    answers = answersDiv.find_all('div', class_='answer')\n",
    "    for answer in answers:\n",
    "        ret.append(get_post(answer))\n",
    "    return ret\n",
    "    \n",
    "def parse_question(link : str)->Dict:\n",
    "    # get the wanted page and soup it\n",
    "    page = requests.get(link)\n",
    "    if page.status_code != 200:\n",
    "        return {}\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    ret = {}\n",
    "    \n",
    "    # get the title of the question and add it to return object\n",
    "    ret['title'] = get_question_title(soup)\n",
    "    \n",
    "    # only work with the mainbar div of the site\n",
    "    mainbar = soup.find('div',id='mainbar')\n",
    "    \n",
    "    # get the question with comments of the page\n",
    "    questionDiv = mainbar.find('div', id='question')\n",
    "    ret['question'] = get_post(questionDiv)\n",
    "    \n",
    "    # get all answers with comments on the current page\n",
    "    answersDiv = mainbar.find('div', id='answers')\n",
    "    ret['answers'] = get_answers(answersDiv)\n",
    "    \n",
    "    # get all answers from the remaining pages\n",
    "    sitePager = mainbar.find('div', class_='pager-answers')\n",
    "    while (sitePager is not None and sitePager.find('a', attrs={'rel': 'next'}) is not None):\n",
    "        page = requests.get('https://stackoverflow.com' + sitePager.find('a', attrs={'rel': 'next'}).get('href'))\n",
    "        if page.status_code != 200:\n",
    "            break\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        mainbar = soup.find('div',id='mainbar')\n",
    "        sitePager = mainbar.find('div', class_='pager-answers')\n",
    "        answersDiv = mainbar.find('div', id='answers')\n",
    "        ret['answers'] = ret['answers'] + get_answers(answersDiv)\n",
    "        \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://stackoverflow.com/questions/12146914/what-is-the-difference-between-linear-regression-and-logistic-regression\"\n",
    "result =  parse_question(url)\n",
    "assert type(result) == dict\n",
    "import json\n",
    "with open('example.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(result, file, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url2 = \"https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result =  parse_question(url2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Bringing it all together (2)\n",
    "\n",
    "Write a function `process_top_questions` that takes a tag and a number `n` that processes the top n questions by votes as explained above. It returns a single list of dicts, one dict for each question. Thereby use the previously defined functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_top_questions(tag : str, n : int, pause = 0.5) -> List[Dict]:\n",
    "    ret = []\n",
    "    questionLinks = get_questions(tag, n)\n",
    "    for questionLink in questionLinks:\n",
    "        ret.append(parse_question(questionLink))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_questions = process_top_questions('data-mining', 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, question in enumerate(top_questions):\n",
    "    with open(f'data_mining{i}.json', 'w', encoding='utf-8') as file:\n",
    "        json.dump(question, file, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Using inverted indices (total of 10 points)\n",
    "It is/was common to use stopword removal and stemming as a preparation for word embeddings. For this task, you should use the stopword list provided by the Python nltk library.\n",
    "\n",
    "## a) The usual preprocessing (2)\n",
    "\n",
    "Write a function `to_tokens` that takes a string and performs tokenization, lowers all tokens, removes stopwords (`'english'`) and performs stemming (`PorterStemmer`). It returns a list of tokens. The tokens are in the same order they were in the initial string. Use the functions from the nltk library.\n",
    "To transform a string into tokens use the function `nltk.word_tokenize`. Do not allow for empty strings or all whitespace strings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mike/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/mike/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "def to_tokens(my_string : str) -> List[str]:\n",
    "    porter = PorterStemmer()\n",
    "    res = [s.lower() for s in nltk.word_tokenize(my_string)]\n",
    "    res = [porter.stem(s) for s in res if not s in stopwords.words('english')]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['write', 'function']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_tokens(\"Write a function\")\n",
    "# ['write', 'function']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['token',\n",
       " 'order',\n",
       " 'initi',\n",
       " 'string',\n",
       " '.',\n",
       " 'use',\n",
       " 'function',\n",
       " 'nltk',\n",
       " 'librari',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_tokens(\"The tokens are in the same order they were in the initial string. Use the functions from the nltk library.\")\n",
    "#['token',\n",
    "# 'order',\n",
    "# 'initi',\n",
    "# 'string',\n",
    "# '.',\n",
    "# 'use',\n",
    "# 'function',\n",
    "# 'nltk',\n",
    "# 'librari',\n",
    "# '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Load data (3)\n",
    "\n",
    "Write a function `load_data` that takes a path to a folder and reads all the `'*.txt'` files inside that folder. It returns a list of file names without the folder containing them and a list of list of strings which contains the file contents preprocessed using the function from a). Use `encoding=\"utf-8\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Set\n",
    "def load_data(folder : str) -> (List[str], List[List[str]]):\n",
    "    file_list = []\n",
    "    texts = []\n",
    "    files = [f for f in Path(folder).glob(\"*.txt\")]\n",
    "    for file in tqdm(files):\n",
    "        file_list.append(file.name)\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as text:\n",
    "            texts.append(to_tokens(text.read()))\n",
    "    return file_list, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"arxiv_snapshot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52b24f3ca6a4d8ca88129f9c90f6bc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# example usage\n",
    "# this may take a while, please remove before submission\n",
    "#files, stemmed_texts = load_data(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-5af6ec69a7dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# could vary for you\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# 0704.1181v1.txt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmed_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# should be same for the same file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# example usage\n",
    "#print(len(files))\n",
    "#print(files[0]) # could vary for you\n",
    "# 0704.1181v1.txt\n",
    "#print(stemmed_texts[0][:20]) # should be same for the same file\n",
    "['simul', 'four-bodi', 'interact', 'nuclear', 'magnet', 'reson', 'quantum', 'inform', 'processor', 'â', 'wen-zhang', 'liu1', ',', 'jin-fu', 'zhang1', ',', 'gui', 'lu', 'long1,2', 'key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(files[-1]) # could vary for you\n",
    "# 1807.07319v2.txt\n",
    "#print(stemmed_texts[-1][:20]) # should be same for the same file\n",
    "['arxiv:1807.07319v2', '[', 'hep-ph', ']', '21', 'mar', '2019', 'model-independ', 'constraint', 'ckm', 'matrix', 'element', '|vtb|', ',', '|vts|', '|vtd|', 'wenx', 'fang1', ',', 'barbara']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Reduce vocabulary (2)\n",
    "Write a function `filter_tokens` that takes a list of list of tokens and an integer  `min_support` as input. It discards all tokens that do not appear in all of the documents at least (`>=`) min_support times. The return values are\n",
    "\n",
    "1) the list of list of tokens. All tokens that appear less than min_support times (in the entire corpus) are __removed__\n",
    "\n",
    "2) a set of tokens that were __not__ removed, i.e. the frequent tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tokens(l_tokens : List[List[str]], min_support : int) -> (List[List[str]], Set[str]):\n",
    "    token_count = {}\n",
    "    for doc in l_tokens:\n",
    "        for token in doc:\n",
    "            if token in token_count:\n",
    "                token_count[token] += 1\n",
    "            else:\n",
    "                token_count[token] = 1\n",
    "    remaining_tokens = set([t for t in token_count.keys() if token_count[t] >= min_support])\n",
    "    new_corpus = [[t for t in doc if t in remaining_tokens] for doc in l_tokens]\n",
    "    return new_corpus, remaining_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example usage\n",
    "#filtered_texts, frequent_tokens = filter_tokens(stemmed_texts, 0)\n",
    "#len(frequent_tokens) #268879"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example usage\n",
    "#filtered_texts, frequent_tokens = filter_tokens(stemmed_texts, 5)\n",
    "#len(frequent_tokens)\n",
    "# 50306"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Write a query function (3)\n",
    "You want to provide a way to quickly search the documents at hand for a combination of search terms. \n",
    "\n",
    "Write a function `query` which gets a string, inverted index dict and a set of frequent tokens. It treats the string as a list of tokens and returns the filenames of all files that include all the search terms in the query string.\n",
    "\n",
    "\n",
    "To test your code, first convert the list of filenames and filtered tokens into an inverted index to get the corresponding filenames for each token. You can use the `to_inverted_index` function provided in the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(search_string : str, inverted_index : Dict[str, Set[str]], frequent_tokens : Set[str]) -> Set[str]:\n",
    "    search = to_tokens(search_string)\n",
    "    #quick check if makes sense to search at all\n",
    "    for t in search:\n",
    "        if t not in frequent_tokens:\n",
    "            return set()\n",
    "    # build set\n",
    "    res = inverted_index[search[0]]\n",
    "    for i,t in enumerate(search):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        res = res.intersection(inverted_index[t])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_inverted_index(extracts : Dict[str, List[str]]) -> Dict[str, Set[str]]:\n",
    "    inverted = {}\n",
    "    for filename in extracts.keys():\n",
    "        for token in extracts[filename]:\n",
    "            if token in inverted:\n",
    "                inverted[token].add(filename)\n",
    "            else:\n",
    "                inverted[token] = set([filename])\n",
    "    return inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#II = to_inverted_index({filename : text for filename, text in zip(files, filtered_texts)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example usage\n",
    "#query(\"computer spider\", II, frequent_tokens)\n",
    "\n",
    "#{'1007.0880v1.txt',\n",
    "# '1410.5760v1.txt',\n",
    "# '1604.02076v1.txt',\n",
    "# '1609.00359v3.txt',\n",
    "# '1610.05332v1.txt',\n",
    "# '1805.03482v2.txt'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example usage\n",
    "#query(\"computer spider man\", II, frequent_tokens)\n",
    "# set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example usage\n",
    "#query(\"gene\", II, frequent_tokens)\n",
    "#{'0712.3383v1.txt',\n",
    "# '0810.4745v1.txt',\n",
    "# '1009.2186v1.txt',\n",
    "# '1101.0211v2.txt',\n",
    "# '1210.6482v1.txt',\n",
    "# '1303.0539v1.txt',\n",
    "# '1408.5461v2.txt',\n",
    "# '1410.0649v1.txt',\n",
    "# '1508.00192v1.txt',\n",
    "# '1510.02516v2.txt',\n",
    "# '1802.02337v1.txt',\n",
    "# '1807.04779v1.txt'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
