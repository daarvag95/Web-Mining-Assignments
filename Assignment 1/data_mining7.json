{"title": "Why is the F-Measure a harmonic mean and not an arithmetic mean of the Precision and Recall measures?", "question": {"text": "\nWhen we calculate the F-Measure considering both Precision and Recall, we take the harmonic mean of the two measures instead of a simple arithmetic mean. \nWhat is the intuitive reason behind taking the harmonic mean and not a simple average?\n", "comments": ["The intuition is to balance precision and recall (usually the best measurement, but in some case you want to maximize precision or recall, which is a different story). You cannot get a high f-score if either one is very low.", " This is a good resource to understanding HM", "Fix the link above:  or the original @"]}, "answers": [{"text": "\nHere we already have some elaborate answers but I thought some more information about it would be helpful for some guys who want to delve deeper(especially why F measure).\nAccording to the theory of measurement, the composite measure should satisfy the following 6 definitions:\n\nConnectedness(two pairs can be ordered) and transitivity(if e1 >= e2 and e2 >= e3 then e1 >= e3)\nIndependence: two components contribute their effects independently to the effectiveness.\nThomsen condition: Given that at a constant recall (precision) we find a difference in effectiveness for two values of precision (recall) then this difference cannot be removed or reversed by changing the constant value.\nRestricted solvability.\nEach component is essential: Variation in one while leaving the other constant gives a variation in effectiveness.\nArchimedean property for each component. It merely ensures that the intervals on a component are comparable.\n\nWe can then  the function of the effectiveness:\n\nAnd normally we don't use the effectiveness but the much simper F score :\n\nNow that we have the general formula of F measure:\n\nwhere we can place more emphasis on recall or precision by setting beta, because beta is defined as follows:\n\nIf we weight recall more important than precision(all relevant are selected) we can set beta as 2 and we get the F2 measure. And if we do the reverse and weight precision higher than recall(as many selected elements are relevant as possible, for instance in some grammar error correction scenarios like ) we just set beta as 0.5 and get the F0.5 measure. And obviously, we can set beta as 1 to get the most used F1 measure(harmonic mean of precision and recall).\nI think to some extent I have already answered why we do not use the arithmetic mean.\nLet's see the 3D plot of the harmonic mean. We can see that the harmonic mean is sensitive to the lowest value, especially the harmonic mean is 0 when at least one is 0 which doesn't hold for the simple arithmetic mean.\n\nFor more visualization of this topic please refer to this article: .\nReferences:\n\n\n\n\n\n\n", "comments": []}, {"text": "\nTo explain, consider for example, what the average of 30mph and 40mph is? if you drive for 1 hour at each speed, the average speed over the 2 hours is indeed the arithmetic average, 35mph.\nHowever if you drive for the same distance at each speed -- say 10 miles -- then the average speed over 20 miles is the harmonic mean of 30 and 40, about 34.3mph.\nThe reason is that for the average to be valid, you really need the values to be in the same scaled units. Miles per hour need to be compared over the same number of hours; to compare over the same number of miles you need to average hours per mile instead, which is exactly what the harmonic mean does.\nPrecision and recall both have true positives in the numerator, and different denominators. To average them it really only makes sense to average their reciprocals, thus the harmonic mean.\n", "comments": ["Thanks, that is a good argument on why this is supported from theory; my answer was more on the pragmatic side."]}, {"text": "\nBecause it punishes extreme values more.\nConsider a trivial method (e.g. always returning class A). There are infinite data elements of class B, and a single element of class A:\n\nWhen taking the arithmetic mean, it would have 50% correct. Despite being the worst possible outcome! With the harmonic mean, the F1-measure is 0.\n\nIn other words, to have a high F1, you need to both have a high precision and recall.\n", "comments": ["When the recall is 0.0 the precision has to be greater than 0.0 right? But I get the point in your example. Nicely explained - Thanks.", "In your example, precision for class A is 0.5 instead of 0 and recall of class A is 1; precision for class B is 0 and recall of class B is 0 as we'll. I assume your balanced class means the true labels are A and B; each applies to 50% of data.", "Let's make infinite elements of class B, and a single element of class A. It doesn't change the math behind F1.", "It is not just a heuristic to select more balance. Harmonic mean is there only way that makes sense given the units of these ratios. Mean wouldn't have a meaning in comparison", "Where does it say \"heuristic\", and where does your comment differ from my answer? But: F-measure is a heuristic in that it assumes precision and recall are equally important. That is why the beta term needs to be chosen - heuristically, one usually uses beta=1."]}, {"text": "\nThe above answers are well explained. This is just for a quick reference to understand the nature of the arithmetic mean and the harmonic mean with plots. As you can see from the plot, consider the X axis and Y axis as precision and recall, and the Z axis as the F1 Score. So, from the plot of the harmonic mean, both the precision and recall should contribute evenly for the F1 score to rise up unlike the Arithmetic mean.\nThis is for the arithmetic mean.\n\nThis is for the Harmonic mean.\n\n", "comments": ["Please use formatting tools to properly edit and format your answer. Image should be displayed here , its not a hyperlink."]}, {"text": "\nThe harmonic mean is the equivalent of the arithmetic mean for reciprocals of quantities that should be averaged by the arithmetic mean. More precisely, with the harmonic mean, you transform all your numbers to the \"averageable\" form (by taking the reciprocal), you take their arithmetic mean and then transform the result back to the original representation (by taking the reciprocal again).\nPrecision and the recall are \"naturally\" reciprocals because their numerator is the same and their denominators are different. Fractions are more sensible to average by arithmetic mean when they have the same denominator.\nFor more intuition, suppose that we keep the number of true positive items constant. Then by taking the harmonic mean of the precision and the recall, you implicitly take the arithmetic mean of the false positives and the false negatives. It basically means that false positives and false negatives are equally important to you when the true positives stay the same. If an algorithm has N more false positive items but N less false negatives (while having the same true positives), the F-measure stays the same.\nIn other words, the F-measure is suitable when:\n\nmistakes are equally bad, whether they are false positives or false negatives\nthe number of mistakes is measured relative to the number of true positives\ntrue negatives are uninteresting\n\nPoint 1 may or may not be true, there are weighted variants of the F-measure that can be used if this assumption isn't true. Point 2 is quite natural since we can expect the results to scale if we just classify more and more points. The relative numbers should stay the same. \nPoint 3 is quite interesting. In many applications negatives are the natural default and it may even be hard or arbitrary to specify what really counts as a true negative. For example a fire alarm is having a true negative event every second, every nanosecond, every time a Planck time has passed etc. Even a piece of rock has these true negative fire-detection events all the time.\nOr in a face detection case, most of the time you \"correctly don't return\" billions of possible areas in the image but this is not interesting. The interesting cases are when you do return a proposed detection or when you should return it.\nBy contrast the classification accuracy cares equally about true positives and true negatives and is more suitable if the total number of samples (classification events) is well-defined and rather small.\n", "comments": []}]}