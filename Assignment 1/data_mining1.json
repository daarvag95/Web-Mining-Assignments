{"title": "Can someone give an example of cosine similarity, in a very simple, graphical way?", "question": {"text": "\n\nCan you show the vectors here (in a list or something)\nand then do the math, and let us see how it works?\nI'm a beginner.\n", "comments": ["Try picking up a copy of Geometry and Meaning by Widdows (), I read it through a while back and wished I had it a number of years ago, great introductory text."]}, "answers": [{"text": "\nHere are two very short texts to compare:\n\n\n\n\nWe want to know how similar these texts are, purely in terms of word counts (and ignoring word order). We begin by making a list of the words from both texts:\n\nNow we count the number of times each of these words appears in each text:\n\nWe are not interested in the words themselves though. We are interested only in\nthose two vertical vectors of counts. For instance, there are two instances of\n'me' in each text. We are going to decide how close these two texts are to each\nother by calculating one function of those two vectors, namely the cosine of\nthe angle between them. \nThe two vectors are, again:\n\nThe cosine of the angle between them is about 0.822.\nThese vectors are 8-dimensional. A virtue of using cosine similarity is clearly\nthat it converts a question that is beyond human ability to visualise to one\nthat can be. In this case you can think of this as the angle of about 35\ndegrees which is some 'distance' from zero or perfect agreement.\n", "comments": ["This is exactly what I was looking for. Exactly.  Is this considered the simplest form of \"vector space model\"?", "I am really glad this was useful to you, Alex. Sorry for the delay in responding. I haven't visited StackOverflow in a while. Actually this is an example of an \"inner product space\". There's a basic discussion on wikipedia.", "Is there any way to normalize for document length?", "You have to use length normalization and before that, try to use log frequency weighting on all term vectors. If your already dealing with normalized vectors, then it's the dot product of A.B.", "More detailed example with use of length normalization and TF-IDF: "]}, {"text": "\nI'm guessing you are more interested in getting some insight into \"why\" the cosine similarity works (why it provides a good indication of similarity), rather than \"how\" it is calculated (the specific operations used for the calculation).  If your interest is in the latter, see the reference indicated by Daniel in this post, as well as .\nTo explain both the how and even more so the why, it is useful, at first, to simplify the problem and to work only in two dimensions. Once you get this in 2D, it is easier to think of it in three dimensions, and of course harder to imagine in many more dimensions, but by then we can use linear algebra to do the numeric calculations and also to help us think in terms of lines / vectors / \"planes\" / \"spheres\" in n dimensions, even though we can't draw these.\nSo, in two dimensions: with regards to text similarity this means that we would focus on two distinct terms, say the words \"London\" and \"Paris\", and we'd count how many times each of these words is found in each of the two documents we wish to compare.  This gives us, for each document, a point in the the x-y plane. For example, if Doc1 had Paris once, and London four times, a point at (1,4) would present this document (with regards to this diminutive evaluation of documents).  Or, speaking in terms of vectors, this Doc1 document would be an arrow going from the origin to point (1,4).  With this image in mind, let's think about what it means for two documents to be similar and how this relates to the vectors.\nVERY similar documents (again with regards to this limited set of dimensions) would have the very same number of references to Paris, AND the very same number of references to London, or maybe, they could have the same ratio of these references. A Document, Doc2, with 2 refs to Paris and 8 refs to London, would also be very similar, only with maybe a longer text or somehow more repetitive of the cities' names, but in the same proportion. Maybe both documents are guides about London, only making passing references to Paris (and how uncool that city is ;-) Just kidding!!!.\nNow, less similar documents may also include references to both cities, but in different proportions. Maybe Doc2 would only cite Paris once and London seven times.\nBack to our x-y plane, if we draw these hypothetical documents, we see that when they are VERY similar, their vectors overlap (though some vectors may be longer), and as they start to have less in common, these vectors start to diverge, to have a wider angle between them.\nBy measuring the angle between the vectors, we can get a good idea of their similarity, and to make things even easier, by taking the Cosine of this angle, we have a nice 0 to 1 or -1 to 1 value that is indicative of this similarity, depending on what and how we account for.  The smaller the angle, the bigger (closer to 1) the cosine value, and also the higher the similarity.\nAt the extreme, if Doc1 only cites Paris and Doc2 only cites London, the documents have absolutely nothing in common.  Doc1 would have its vector on the x-axis, Doc2 on the y-axis, the angle 90 degrees, Cosine 0. In this case we'd say that these documents are orthogonal to one another.\nAdding dimensions:\nWith this intuitive feel for similarity expressed as a small angle (or large cosine), we can now imagine things in 3 dimensions, say by bringing the word \"Amsterdam\" into the mix, and visualize quite well how a document with two references to each would have a vector going in a particular direction, and we can see how this direction would compare to a document citing Paris and London three times each, but not Amsterdam, etc. As said, we can try and imagine the this fancy space for 10 or 100 cities. It's hard to draw, but easy to conceptualize.\nI'll wrap up just by saying a few words about the formula itself. As I've said, other references provide good information about the calculations.\nFirst in two dimensions. The formula for the Cosine of the angle between two vectors is derived from the trigonometric difference (between angle a and angle b):\n\nThis formula looks very similar to the dot product formula:\n\nwhere  corresponds to the  value and  the  value, for the first vector, etc.  The only problem, is that , , etc. are not exactly the  and  values, for these values need to be read on the unit circle. That's where the denominator of the formula kicks in: by dividing by the product of the length of these vectors, the  and  coordinates become normalized.\n", "comments": []}, {"text": "\nHere's my implementation in C#.\n\n", "comments": ["this is awesome thank you I loved how you explained Magnitude =)", "That's great but what if we are working with files or strings."]}, {"text": "\nFor simplicity I am reducing the vector a and b:\n\nThen cosine similarity (Theta):\n\nthen inverse of cos 0.5 is 60 degrees.\n", "comments": []}, {"text": "\nThis Python code is my quick and dirty attempt to implement the algorithm:\n\n", "comments": ["Can you explain why you used set in the line \"all_items = set(counter1.keys()).union(set(counter2.keys()))\".", "@Ghos3t , that is to get list of distinct words from both documents"]}, {"text": "\nUsing @Bill Bell example, two ways to do this in [R]\n\nor taking advantage of crossprod() method's performance...\n\n", "comments": []}, {"text": "\nThis is a simple  code which implements cosine similarity.\n\n", "comments": []}, {"text": "\n\n", "comments": []}, {"text": "\nSimple JAVA code to calculate cosine similarity\n\n", "comments": ["It's not a \"simple, graphical way\" but yet just code. Although others have made the same error too :/"]}, {"text": "\nTwo Vectors A and B exists in a 2D space or 3D space, the angle between those vectors is cos similarity. \nIf the angle is more (can reach max 180 degree) which is Cos 180=-1 and the minimum angle is 0 degree. cos 0 =1 implies the vectors are aligned to each other and hence the vectors are similar. \ncos 90=0 (which is sufficient to conclude that the vectors A and B are not similar at all and since distance cant be negative, the cosine values will lie from 0 to 1. Hence, more angle implies implies reducing similarity (visualising also it makes sense)\n", "comments": []}, {"text": "\nHere's a simple Python code to calculate cosine similarity:\n\n", "comments": []}]}