{"title": "1D Number Array Clustering", "question": {"text": "\nSo let's say I have an array like this:\n\nIs there a convenient way to partition the array into something like this?\n\nI looked through similar questions yet most people suggested using k-means to cluster points, like , which is quite confusing to use for a beginner like me. Also I think that k-means is more suitable for two or more dimensional clustering right? Are there any ways to partition an array of N numbers to many partitions/clustering depending on the numbers?\nSome people also suggest rigid range partitioning, but it doesn't always render the results as\n expected\n", "comments": []}, "answers": [{"text": "\nDon't use multidimensional clustering algorithms for a one-dimensional problem. A single dimension is much more special than you naively think, because you can actually sort it, which makes things a lot easier.\nIn fact, it is usually not even called clustering, but e.g. segmentation or natural breaks optimization.\nYou might want to look at  and similar statistical methods.  is also a good method to look at, with a strong statistical background. Local minima in density are be good places to split the data into clusters, with statistical reasons to do so. KDE is maybe the most sound method for clustering 1-dimensional data.\nWith KDE, it again becomes obvious that 1-dimensional data is much more well behaved. In 1D, you have local minima; but in 2D you may have saddle points and such \"maybe\" splitting points. See this , as how such a point may or may not be appropriate for splitting clusters.\nSee  for an example how to do this in Python (green markers are the cluster modes; red markers a points where the data is cut; the y axis is a log-likelihood of the density):\n\n", "comments": ["Implementation here: ", "Could you update your answer with why  or  may or may not be good approaches to clustering 1D? See ", "Essentially, both are very naive approximations to Kernel Density Estimation. Mean-Shift is a mode-seeking approach for multivariate KDE, and DBSCAN is using the most primitive KDE (box kernel) to define what is dense and what is not. There is 0 benefit to use them on 1-dimensional data.", "Ckmeans.1d.dp (k-means adapted for dimensional clustering) is worth a look however. See ", "@skoush that is a slower k-means variant that yields the global optimum (in 1d only). But if the SSQ k-means objective doesn't solve your problem it does not matter if you find a 0.1% better (by SSQ) k-means solution than with the faster standard algorithm."]}, {"text": "\nYou may look for discretize algorithms. 1D discretization  problem is a lot similar to what you are asking. They decide cut-off points, according to frequency, binning strategy etc.\n uses following algorithms in its , discretization process.\n\nweka.filters.supervised.attribute.Discretize\nuses either Fayyad & Irani's MDL method or Kononeko's MDL criterion\nweka.filters.unsupervised.attribute.Discretize\nuses simple binning\n\n", "comments": ["Hi! The link doesn't seem accessible anymore.. do you have another resource please?", "@MJimitater Fixed link"]}]}