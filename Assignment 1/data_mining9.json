{"title": "Mixing categorial and continuous data in Naive Bayes classifier using scikit-learn", "question": {"text": "\nI'm using scikit-learn in Python to develop a classification algorithm to predict the gender of certain customers. Amongst others, I want to use the Naive Bayes classifier but my problem is that I have a mix of categorical data (ex: \"Registered online\", \"Accepts email notifications\" etc) and continuous data (ex: \"Age\", \"Length of membership\" etc). I haven't used scikit much before but I suppose that that Gaussian Naive Bayes is suitable for continuous data and that Bernoulli Naive Bayes can be used for categorical data. However, since I want to have both categorical and continuous data in my model, I don't really know how to handle this. Any ideas would be much appreciated!\n", "comments": ["Hi, can you tell which solution worked best for you?"]}, "answers": [{"text": "\nYou have at least two options:\n\nTransform all your data into a categorical representation by computing percentiles for each continuous variables and then binning the continuous variables using the percentiles as bin boundaries. For instance for the height of a person create the following bins: \"very small\", \"small\", \"regular\", \"big\", \"very big\" ensuring that each bin contains approximately 20% of the population of your training set. We don't have any utility to perform this automatically in scikit-learn but it should not be too complicated to do it yourself. Then fit a unique multinomial NB on those categorical representation of your data.\nIndependently fit a gaussian NB model on the continuous part of the data and a multinomial NB model on the categorical part. Then transform all the dataset by taking the class assignment probabilities (with  method) as new features:  and then refit a new model (e.g. a new gaussian NB) on the new features.\n\n", "comments": ["@ogrisel: Am I right in believing that the second method might miss correlations between the continuous and categorical data? For example, suppose young people who register online are typically male, but young people who do not register online are typically female. But further suppose for the sake of concreteness that the gaussian NB model predicts young people (without knowledge of the categorical data) are generally male. Since only this probability is being passed on to the second-stage gaussian NB, it will miss the correlation.", "@unutbu: Naive Bayes classifiers assumes independence of the features given the class.  The first method listed above will learn  and  independently.  The correlation between age and registration_type will not be captured for a given gender.", "@ogrisel can we use one-hot-encoding to convert the categorical variables to  values between 0 and n-1 for n classes and keep the continuous variables as they are for GaussianNB() ? based on this post: ", "@jai, No! First, one-hot encoding is not the same as converting to values between 0 and n-1.  Second, converting categorical variables to values between 0 and n-1 and then treating them as continuous variables makes no sense.  Third, one-hot categorical variables are so non-Gaussian, that treating them as Gaussian (which  assumes) does not, in my experience, produce good results.", "@ogrisel, I thought that  is to be used for predicting probabilities on the 'test' data. E.g. I create 2 separate classifiers on the train data, and I can then use this to predict the probability for my remaining  data. If I then train another Gaussian model on the  result from the  data, doesn't that leave with nothing to test on? Am I looking at this correctly? Cheers"]}, {"text": "\nThe simple answer: multiply result!! it's the same.\nNaive Bayes based on applying Bayes’ theorem with the “naive” assumption of independence between every pair of features - meaning you calculate the Bayes probability dependent on a specific feature without holding the others - which means that the algorithm multiply each probability from one feature with the probability from the second feature (and we totally ignore the denominator - since it is just a normalizer).\nso the right answer is:\n\ncalculate the probability from the categorical variables.\ncalculate the probability from the continuous variables.\nmultiply 1. and 2. \n\n", "comments": ["Gaussian NB gives a density estimate for the prior. I'm not sure about what you meant for the second part.", "@Davis, I'm not sure what you meant, but the Gaussian NB means that the the likelihood of the features is assumed to be Gaussian and this how the P(x|y) is calculated.", "I mean there isn't Pr(x_i | y) anymore, but this prior is replaced Norm(mu_i, sig_i) which is a density estimate because the probability of Pr(X_i = x | y) is zero as the RV X_i is continuous.", "I think your question is not related to the topic but you can get your answer from: ", "Yaron ,when you say , is this on the test or train data, and what function would you use to do this? Are you using  on the train data as well as doing the fit on the train data? I'm struggling to figure out what I should be multiplying... Cheers"]}, {"text": "\nHope I'm not too late. I recently wrote a library called Mixed Naive Bayes, written in NumPy. It can assume a mix of Gaussian and categorical (multinoulli) distributions on the training data features.\n\nThe  library is written such that the APIs are similar to 's.\nIn the example below, let's assume that the first 2 features are from a categorical distribution and the last 2 are Gaussian. In the  method, just specify , indicating that Columns 0 and 1 are to follow categorical distribution.\n\nPip installable via . More information on the usage in the README.md file. Pull requests are greatly appreciated :)\n", "comments": []}]}