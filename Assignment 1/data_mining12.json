{"title": "Calculate AUC in R?", "question": {"text": "\nGiven a vector of scores and a vector of actual class labels, how do you calculate a single-number AUC metric for a binary classifier in the R language or in simple English? \nPage 9 of  seems to require knowing the class labels, and here is  where I don't understand \n\nBecause R (not to be confused with the R language) is defined a vector but used as a function?\n", "comments": ["For anyone else who doesn't know, apparently AUC is the \"Area Under the  Curve\""]}, "answers": [{"text": "\nAs mentioned by others, you can compute the AUC using the  package.  With the ROCR package you can also plot the ROC curve, lift curve and other model selection measures. \nYou can compute the AUC directly without using any package by using the fact that the AUC is equal to the probability that a true positive is scored greater than a true negative.\nFor example, if  is a vector containing a score of the positive examples, and  is a vector containing the negative examples then the AUC is approximated by:\n\nwill give an approximation of the AUC.  You can also estimate the variance of the AUC by bootstrapping:\n\n", "comments": ["For my test data set your replicated value is very similar to @jonw's (is 0.8504, yours 0.850591) except I don't need to install pROC.  Thank you", "@Andrew @eric This is a terrible answer. You do NOT estimate the variance of the AUC - you only estimate the variance of the resampling process. To convince yourself, try changing the sample size in ... divide it by 10, your variance is multiplied by 10. Multiply it by 10 and your variance is divided by 10. This is certainly not the desired behaviour to compute the variance of the AUC.", "In addition the answer should note that the estimate is as good as the number of replicates. Go to infinity and you get the actual AUC.", "Agree with @Calimo, that is not a bootstrap.  To bootstrap you have to resample N data points with replacement M times, where N is the total size of the original data set and M can be whatever (usually a couple hundred or more).  N is not arbitrary.  If N is not set to the full data set size you'll get biased statistics.", "I'm a bit unclear on the base R method shown. Can it be calculated purely from the confusion matrix? In the context of a given confusion matrix, what would  and  be?"]}, {"text": "\n will calculate the AUC among other statistics:\n\n", "comments": ["I've used ROCR for plotting performance, but I don't see how it calculates a \"single-number AUC metric\" (from the original question).", ""]}, {"text": "\nWith the package  you can use the function  like this example from the help page:\n\n", "comments": []}, {"text": "\nWithout any additional packages:\n\n\n", "comments": ["If you copy-paste this code and receive , it's probably because your labels are 0-1, while @AGS is using labels 1-2.", "It doesn't give the true AUC if two observations have the same probability and the order of the observation is not random. Otherwise nice and fast code.", "Do not know why this solution does not work on my data, my probs are not normalized to be within [0,1]"]}, {"text": "\nI found some of the solutions here to be slow and/or confusing (and some of them don't handle ties correctly) so I wrote my own  based function  in my R package .\n\n", "comments": ["This solution is much much faster than auc() method in pROC package!  auc() method in pROC package is pretty slow if one has to calculate auc scores for multi-class or multiple output regression problem."]}, {"text": "\nYou can learn more about AUROC in this blog post by :\n\nHe provides a fast function for AUROC:\n\nLet's test it:\n\n is 100 times faster than  and .\n is 10 times faster than  and .\n\n", "comments": ["For larger sample sizes,  is even faster (implemented in C++). Disclaimer: I'm the author."]}, {"text": "\nCombining code from , along with @J. Won.'s answer to this question and a few more places, the following plots the ROC curve and prints the AUC in the bottom right on the plot.\nBelow  is a numeric vector of predicted probabilities for binary classification and  contains the true labels of the test data.\n\nThis gives a plot like this:\n\n", "comments": []}, {"text": "\nI usually use the function  from the DiagnosisMed package. I like the graph it produces. AUC is returned along with it's confidence interval and it is also mentioned on the graph.\n\n", "comments": ["As of July 20, 2016 this link  says "]}, {"text": "\nAlong the lines of erik's response, you should also be able to calculate the ROC directly by comparing all possible pairs of values from pos.scores and neg.scores:\n\nCertainly less efficient than the sample approach or the pROC::auc, but more stable than the former and requiring less installation than the latter.\nRelated: when I tried this it gave similar results to pROC's value, but not exactly the same (off by 0.02 or so); the result was closer to the sample approach with very high N. If anyone has ideas why that might be I'd be interested.\n", "comments": ["One source of inaccuracy is dealing with ties. Technically you should take the probability that the positive case score is strictly greater than the negative score + 1/2 * prob they are equal. If all scores are unique this won't be a problem."]}, {"text": "\nCurrently top voted answer is incorrect, because it disregards ties. When positive and negative scores are equal, then AUC should be 0.5. Below is corrected example.\n\n", "comments": []}, {"text": "\nCalculating AUC with   package is very easy and straightforward:\n\n", "comments": []}]}