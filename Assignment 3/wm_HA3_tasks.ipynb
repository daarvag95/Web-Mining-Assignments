{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web-Mining - Assignment 3 (25 points total)\n",
    "\n",
    "This **Home Assignment** is to be submitted and you will be given points for each of the tasks. It familiarizes you with the Apriori-algorithm, Association Rule Mining and Matrix factorization.\n",
    "You can expect the following packages to be installed: numpy, pandas, matplotlib and of course the python standard library.\n",
    "\n",
    "## Formalities\n",
    "**Submit in a group of 3-4 people until 29.06.2021 23:59CET. The deadline is strict!**\n",
    "\n",
    "## Evaluation and Grading\n",
    "General advice for programming excercises at *CSSH*:\n",
    "Evaluation of your submission is done semi automatically. Think of it as this notebook being \n",
    "executed once. Afterwards, some test functions are appended to this file and executed respectively.\n",
    "\n",
    "Therefore:\n",
    "* Submit valid _Python3_ code only!\n",
    "* Use external libraries only when specified by task.\n",
    "* Ensure your definitions (functions, classes, methods, variables) follow the specification if\n",
    "  given. The concrete signature of e.g. a function usually can be inferred from task description, \n",
    "  code skeletons and test cases.\n",
    "* Ensure the notebook does not rely on current notebook or system state!\n",
    "  * Use `Kernel --> Restart & Run All` to see if you are using any definitions, variables etc. that \n",
    "    are not in scope anymore.\n",
    "  * Double check if your code relies on presence of files or directories other than those mentioned\n",
    "    in given tasks. Tests run under Linux, hence don't use Windows style paths \n",
    "    (`some\\path`, `C:\\another\\path`). Also, use paths only that are relative to and within your\n",
    "    working directory (OK: `some/path`, `./some/path`; NOT OK: `/home/alice/python`, \n",
    "    `../../python`).\n",
    "* Keep your code idempotent! Running it or parts of it multiple times must not yield different\n",
    "  results. Minimize usage of global variables.\n",
    "* Ensure your code / notebook terminates in reasonable time.\n",
    "\n",
    "**There's a story behind each of these points! Don't expect us to fix your stuff!**\n",
    "\n",
    "Regarding the scores, you will get no points for a task if:\n",
    "- your function throws an unexpected error (e.g. takes the wrong number of arguments)\n",
    "- gets stuck in an infinite loop\n",
    "- takes much much longer than expected (e.g. >1s to compute the mean of two numbers)\n",
    "- does not produce the desired output (e.g. returns an descendingly sorted list even though we asked for ascending, returns the mean and the std even though we asked for only the mean, prints an output instead of returning it, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credentials of all team members (you may add or remove items from the dictionary)\n",
    "team_members = [\n",
    "    {\n",
    "        'first_name': 'Pruthvi',\n",
    "        'last_name': 'Hegde',\n",
    "        'student_id': 404809\n",
    "    },\n",
    "    {\n",
    "        'first_name': 'Mike',\n",
    "        'last_name': 'Gr√ºne',\n",
    "        'student_id': 381076\n",
    "    },\n",
    "    {\n",
    "        'first_name': 'Seyed Pouria',\n",
    "        'last_name': 'Mirelmi',\n",
    "        'student_id': 416910\n",
    "    },\n",
    "    {\n",
    "        'first_name': 'Haron',\n",
    "        'last_name': 'Nqiri',\n",
    "        'student_id': 343289\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Apriori (18 points)\n",
    "\n",
    "In this task you should implement a general version of the apriori algorithm.\n",
    "\n",
    "The main idea of the apriori principle is, that if for a constraint you observe one property drops below a threshold (e.g. support drops below minimal support), it will never exceed that threshold for any more specific pattern.\n",
    "\n",
    "Example for support:\n",
    "$$ \\sigma( \\{\\text{apple}, \\text{banana}\\}) < t \\Rightarrow \\sigma( \\{\\text{apple}, \\text{banana}, \\text{any_other_item}\\}) < t$$\n",
    "\n",
    "If any function $f$ fullfills\n",
    "\n",
    "$$ f( \\{\\text{apple}, \\text{banana}\\}) < t \\Rightarrow f( \\{\\text{apple}, \\text{banana}, \\text{any_other_item}\\}) < t$$\n",
    " we call this property *anti-monotone*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task a pattern will be a **sorted** tuple of items e.g. `('apple', 'banana', 'yoghurt')`. Items are just strings. Assume the database is given as a dataframe, each row representing a single transaction. It also has a column `'items'` that contains the items for that transaction as a set.\n",
    "\n",
    "For an example database see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "df = pd.read_csv (\"weblog.csv\", na_values=\"?\")\n",
    "\n",
    "# nice and short solution\n",
    "def collect_websites(df) -> defaultdict(set):\n",
    "    history = defaultdict(set)\n",
    "    for user, website in zip(df.user_id, df.website_area):\n",
    "        history[user].add(website)\n",
    "    return pd.DataFrame.from_records(list(history.items()), columns=(\"user_id\", \"items\") )\n",
    "    return history\n",
    "\n",
    "websites_db = collect_websites(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>items</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>{Support Desktop, End User Produced View, regwiz}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10002</td>\n",
       "      <td>{Support Desktop, Knowledge Base}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10003</td>\n",
       "      <td>{Support Desktop, Knowledge Base, Microsoft.co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10004</td>\n",
       "      <td>{Norway}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10005</td>\n",
       "      <td>{misc}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                              items\n",
       "0    10001  {Support Desktop, End User Produced View, regwiz}\n",
       "1    10002                  {Support Desktop, Knowledge Base}\n",
       "2    10003  {Support Desktop, Knowledge Base, Microsoft.co...\n",
       "3    10004                                           {Norway}\n",
       "4    10005                                             {misc}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "websites_db.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Projecting a database (1)\n",
    "Write a function `project_database(database, pattern)` that returns all rows of the `database` that contain all items in `pattern`. You can assume, that the database has a column `\"items\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import pandas as pd\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_database(database : pd.DataFrame, pattern : Tuple[str]) -> pd.DataFrame:\n",
    "    ps = set(pattern)\n",
    "    newDB = database.loc[database['items'].apply(lambda x: ps.issubset(x))]\n",
    "    for index, row in newDB.iterrows():\n",
    "        newDB.at[index,'items'] = tuple(sorted(row['items']))\n",
    "    return newDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>items</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{milk, apple, banana}</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{apple, banana, citron}</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{apple, citron}</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{apple, citron}</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{apple}</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{milk}</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     items  price\n",
       "0    {milk, apple, banana}     10\n",
       "1  {apple, banana, citron}     20\n",
       "2          {apple, citron}     13\n",
       "3          {apple, citron}     13\n",
       "4                  {apple}      5\n",
       "5                   {milk}      3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example database\n",
    "items = [{'apple', 'banana', 'milk'},\n",
    "      {'apple', 'banana', 'citron'},\n",
    "      {'apple', 'citron'},\n",
    "      {'apple', 'citron'},\n",
    "      {'apple',},\n",
    "      {'milk',}]\n",
    "\n",
    "prices = [10, 20, 13, 13, 5, 3]\n",
    "example_db = pd.DataFrame.from_dict({'items':items, 'price' : prices})\n",
    "example_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>items</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(apple, banana, milk)</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(apple, banana, citron)</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(apple, citron)</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(apple, citron)</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(apple,)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(milk,)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     items  price\n",
       "0    (apple, banana, milk)     10\n",
       "1  (apple, banana, citron)     20\n",
       "2          (apple, citron)     13\n",
       "3          (apple, citron)     13\n",
       "4                 (apple,)      5\n",
       "5                  (milk,)      3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>items</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(apple, banana, citron)</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(apple, citron)</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(apple, citron)</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     items  price\n",
       "1  (apple, banana, citron)     20\n",
       "2          (apple, citron)     13\n",
       "3          (apple, citron)     13"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>items</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(apple, banana, milk)</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(apple, banana, citron)</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(apple, citron)</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(apple, citron)</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(apple,)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     items  price\n",
       "0    (apple, banana, milk)     10\n",
       "1  (apple, banana, citron)     20\n",
       "2          (apple, citron)     13\n",
       "3          (apple, citron)     13\n",
       "4                 (apple,)      5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>items</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(apple, banana, citron)</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     items  price\n",
       "1  (apple, banana, citron)     20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(project_database(example_db, tuple()))\n",
    "display(project_database(example_db, (\"apple\", \"citron\")))\n",
    "display(project_database(example_db, (\"apple\", )))# notice comma\n",
    "display(project_database(example_db, (\"apple\", \"banana\", \"citron\")))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\titems \tprice\n",
    "#0 \t(apple, banana, milk) \t10\n",
    "#1 \t(apple, banana, citron) \t20\n",
    "#2 \t(apple, citron) \t13\n",
    "#3 \t(apple, citron) \t13\n",
    "#4 \t(apple,) \t5\n",
    "#5 \t(milk,) \t3\n",
    "\n",
    "#\titems \tprice\n",
    "#1 \t(apple, banana, citron) \t20\n",
    "#2 \t(apple, citron) \t13\n",
    "#3 \t(apple, citron) \t13\n",
    "\n",
    "#\titems \tprice\n",
    "#0 \t(apple, banana, milk) \t10\n",
    "#1 \t(apple, banana, citron) \t20\n",
    "#2 \t(apple, citron) \t13\n",
    "#3 \t(apple, citron) \t13\n",
    "#4 \t(apple,) \t5\n",
    "\n",
    "#\titems \tprice\n",
    "#1 \t(apple, banana, citron) \t20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Calculating the support/average price (1+1)\n",
    "Write a function `calc_support(database, pattern)` that computes the support for the `pattern` in the `database`. Thereby use the `project_database` function defined in a).\n",
    "\n",
    "Write another function `calc_average_price(database, pattern)` that calculates the average price for the pattern.\n",
    "\n",
    "\n",
    "Make sure you are using the function defined in a). Notice that `calc_average_price` is **not** anti-monotone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Tuple, List, Iterable\n",
    "def calc_support(database : pd.DataFrame, pattern : Tuple[str]) -> float:\n",
    "    no_of_transactions = len(database.index)\n",
    "\n",
    "    proj_database = project_database(database,pattern)\n",
    "    no_of_matched_transactions = len(proj_database.index)\n",
    "    support = no_of_matched_transactions/no_of_transactions\n",
    "    return support\n",
    "    \n",
    "    \n",
    "def calc_average_price(database : pd.DataFrame, pattern : Tuple[str]) -> float:\n",
    "    proj_database = project_database(database,pattern)\n",
    "    price = proj_database['price'].values\n",
    "    total = 0\n",
    "    for i in price:\n",
    "        total = total + int(i)\n",
    "    avg = total / len(price)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.5\n",
      "0.8333333333333334\n",
      "0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "print(calc_support(example_db, tuple()))\n",
    "print(calc_support(example_db, (\"apple\", \"citron\")))\n",
    "print(calc_support(example_db, (\"apple\", )))# notice comma\n",
    "print(calc_support(example_db, (\"apple\", \"banana\", \"citron\")))\n",
    "\n",
    "#1.0\n",
    "#0.5\n",
    "#0.8333333333333334\n",
    "#0.16666666666666666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.666666666666666\n",
      "15.333333333333334\n",
      "12.2\n",
      "20.0\n"
     ]
    }
   ],
   "source": [
    "print(calc_average_price(example_db, tuple()))\n",
    "print(calc_average_price(example_db, (\"apple\", \"citron\")))\n",
    "print(calc_average_price(example_db, (\"apple\", )))# notice comma\n",
    "print(calc_average_price(example_db, (\"apple\", \"banana\", \"citron\")))\n",
    "\n",
    "#10.666666666666666\n",
    "#15.333333333333334\n",
    "#12.2\n",
    "#20.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Generating initial candidates (1+2)\n",
    "We want to minimize the number of times that we need to scan the database (or in our case to \"project\" the database) as much as possible. Therefore, we want to use the apriori principle as much as possible.\n",
    "\n",
    "To that extent, first generate *initial candidates* of length $l+1$ from all the frequent patterns of length $l$ (this task) which are then further pruned by using the apriori principle (next task).\n",
    "\n",
    "#### a) The simple way (1)\n",
    "The easy way is to collect all items that are in all the patterns and consider all combinations of those items of length $l+1$. It has runtime $O(\\binom{I}{l+1})$. $\\binom{x}{y}$ meaning the binomial coeff. $I$ is the number of items.\n",
    "\n",
    "\n",
    "#### b) The faster way (2)\n",
    "You can gain significant speedup in this step by only generating initial candidates of length $l+1$ that arise when matching pairs of frequent patterns where the first $l-1$ items are the same. Implement this strategy as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubTuples(itemset, n):\n",
    "    return itertools.combinations(itemset, n)\n",
    "\n",
    "def generate_patterns_a(pattern: List[Tuple[str]]) -> Iterable[Tuple[str]]:\n",
    "    items = set()\n",
    "    if (len(pattern) != 0):\n",
    "        for i in pattern:\n",
    "            l = len(i) + 1\n",
    "            for j in i:\n",
    "                items.add(j)\n",
    "\n",
    "        subsets = getSubTuples(sorted(items), l)\n",
    "    else:\n",
    "        subsets = []\n",
    "    return subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<itertools.combinations at 0x7f2a0c0b71d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example usage\n",
    "generate_patterns_a([(\"a\", \"b\"), (\"a\", \"c\"), (\"c\", \"d\")])\n",
    "# <generator object generate_patterns_a at 0x00000...\n",
    "# <itertools.combinations at 0x1...\n",
    "# Could be anything that is iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# [('a', 'b', 'c'), ('a', 'b', 'd'), ('a', 'c', 'd'), ('b', 'c', 'd')]\n",
      "# [('a', 'b', 'c'), ('a', 'b', 'd'), ('a', 'b', 'e'), ('a', 'c', 'd'), ('a', 'c', 'e'), ('a', 'd', 'e'), ('b', 'c', 'd'), ('b', 'c', 'e'), ('b', 'd', 'e'), ('c', 'd', 'e')]\n",
      "# []\n",
      "# [('a', 'b', 'c'), ('a', 'b', 'd'), ('a', 'b', 'e'), ('a', 'c', 'd'), ('a', 'c', 'e'), ('a', 'd', 'e'), ('b', 'c', 'd'), ('b', 'c', 'e'), ('b', 'd', 'e'), ('c', 'd', 'e')]\n"
     ]
    }
   ],
   "source": [
    "# example usage\n",
    "# notice the items in the patterns are still in correct order\n",
    "print(\"#\", list(generate_patterns_a([(\"a\", \"b\"), (\"a\", \"c\"), (\"c\", \"d\")])))\n",
    "print(\"#\", list(generate_patterns_a([(\"a\", \"b\"), (\"a\", \"c\"), (\"c\", \"d\"), (\"c\", \"e\")])))\n",
    "print(\"#\", list(generate_patterns_a([])))\n",
    "print(\"#\", list(generate_patterns_a([(\"a\", \"b\"), (\"a\", \"c\"), (\"c\", \"d\"), (\"c\", \"e\"), (\"a\", \"d\")])))\n",
    "# [('a', 'b', 'c'), ('a', 'b', 'd'), ('a', 'c', 'd'), ('b', 'c', 'd')]\n",
    "# [('a', 'b', 'c'), ('a', 'b', 'd'), ('a', 'b', 'e'), ('a', 'c', 'd'), ('a', 'c', 'e'), ('a', 'd', 'e'), ('b', 'c', 'd'), ('b', 'c', 'e'), ('b', 'd', 'e'), ('c', 'd', 'e')]\n",
    "# []\n",
    "# [('a', 'b', 'c'), ('a', 'b', 'd'), ('a', 'b', 'e'), ('a', 'c', 'd'), ('a', 'c', 'e'), ('a', 'd', 'e'), ('b', 'c', 'd'), ('b', 'c', 'e'), ('b', 'd', 'e'), ('c', 'd', 'e')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_patterns_b(pattern : List[Tuple[str]]) -> List[Tuple[str]]:\n",
    "    s = set()\n",
    "    if len(pattern) != 0 :\n",
    "        l = len(pattern)\n",
    "        for i in range(l):\n",
    "            for j in range(i+1, l):\n",
    "                item_set = set()\n",
    "                iden = True\n",
    "                for r in range(len(pattern[i])-1):\n",
    "                    if pattern[i][r] != pattern[j][r]:\n",
    "                        iden = False\n",
    "                        continue\n",
    "                if iden is True:\n",
    "                    item_set.update(pattern[i]+pattern[j])\n",
    "                    s.add(tuple(sorted(item_set)))\n",
    "        li = list(s)\n",
    "        li.sort(key=lambda tup: tup[1])\n",
    "    else:\n",
    "        li = []\n",
    "    return li\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# [('a', 'b', 'c')]\n",
      "# [('a', 'b', 'c'), ('c', 'd', 'e')]\n",
      "# []\n",
      "# [('a', 'b', 'c'), ('a', 'b', 'd'), ('a', 'c', 'd'), ('c', 'd', 'e')]\n"
     ]
    }
   ],
   "source": [
    "# example usage\n",
    "# Hope this already makes clear why this is the better approach\n",
    "print(\"#\", list(generate_patterns_b([(\"a\", \"b\"), (\"a\", \"c\"), (\"c\", \"d\")])))\n",
    "print(\"#\", list(generate_patterns_b([(\"a\", \"b\"), (\"a\", \"c\"), (\"c\", \"d\"), (\"c\", \"e\")])))\n",
    "print(\"#\", list(generate_patterns_b([])))\n",
    "print(\"#\", list(generate_patterns_b([(\"a\", \"b\"), (\"a\", \"c\"), (\"c\", \"d\"), (\"c\", \"e\"), (\"a\", \"d\")])))\n",
    "# [('a', 'b', 'c')]\n",
    "# [('a', 'b', 'c'), ('c', 'd', 'e')]\n",
    "# []\n",
    "# [('a', 'b', 'c'), ('a', 'b', 'd'), ('a', 'c', 'd'), ('c', 'd', 'e')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Generate next level candidates (2)\n",
    "\n",
    "We provide a function `generate_next_level_candidates` that takes a list of patterns (assume that the constraint holds for all the patterns that are supplied) and a list of candidate patterns. It returns all the candidates that need to be checked against the database. There is one part of this function missing, the function `prune_candidates`. This function should further prune the *initial candidates* obtained in the previous step.\n",
    "\n",
    "The function `prune_candidates` `yield`s all patterns of length `l+1` from the candidate patterns that are not already excluded through the apriori principle. I.e. a candidate of length $l+1$ is returned if **all** sub-patterns of length `l` are in `patterns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_candidates(patterns : List[Tuple[str]], candidates : Iterable[Tuple[str]]) -> Iterable[Tuple[str]]:\n",
    "    # Notice, that the length of candidates might not be available\n",
    "    if len(patterns) != 0:\n",
    "        s = set()\n",
    "        for can in candidates:\n",
    "            i = True\n",
    "            for sub in getSubTuples(can, len(can) - 1):\n",
    "                if sub not in patterns:\n",
    "                    i = False\n",
    "            if i is True:\n",
    "                s.add(can)\n",
    "        li = list(s)\n",
    "        yield from li\n",
    "    else:\n",
    "        yield from ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# [('a', 'b'), ('b', 'c')]\n",
      "# []\n",
      "# [('a', 'b', 'c')]\n",
      "# []\n",
      "# [('a', 'b', 'c', 'd')]\n"
     ]
    }
   ],
   "source": [
    "print(\"#\", list(prune_candidates([(\"a\",), (\"b\",), (\"c\",)], [(\"a\", \"b\"), (\"b\", \"c\"), (\"e\", \"f\")])))\n",
    "print(\"#\", list(prune_candidates([(\"a\", \"b\"), (\"a\", \"c\"), (\"c\", \"d\")], [(\"a\", \"b\", \"c\")])))\n",
    "print(\"#\", list(prune_candidates([(\"a\", \"b\"), (\"a\", \"c\"), (\"b\", \"c\")], [(\"a\", \"b\", \"c\")])))\n",
    "print(\"#\", list(prune_candidates([(\"a\", \"b\", \"c\"), (\"a\", \"b\", \"d\"), (\"a\", \"c\", \"d\")], [(\"a\", \"b\", \"c\", \"d\")])))\n",
    "print(\"#\", list(prune_candidates([(\"a\", \"b\", \"c\"), (\"a\", \"b\", \"d\"), (\"a\", \"c\", \"d\"), (\"b\", \"c\", \"d\")], [(\"a\", \"b\", \"c\", \"d\")])))\n",
    "\n",
    "# [('a', 'b'), ('b', 'c')]\n",
    "# []\n",
    "# [('a', 'b', 'c')]\n",
    "# []\n",
    "# [('a', 'b', 'c', 'd')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! IMPORTANT !!!\n",
    "generate_patterns = generate_patterns_b\n",
    "# do not forget to change to _a if you don't manage generate_patterns_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_level_candidates(patterns : List[Tuple[str]]) -> Iterable[Tuple[str]]:\n",
    "    if len(patterns) == 0:\n",
    "        return []\n",
    "    level = len(patterns[0])\n",
    "    \n",
    "    resulting_patterns = []\n",
    "    candidates = generate_patterns(patterns)\n",
    "    return prune_candidates(patterns,  candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# [('a', 'c'), ('a', 'b'), ('b', 'c')]\n",
      "# []\n",
      "# []\n",
      "# [('a', 'b', 'c', 'd')]\n"
     ]
    }
   ],
   "source": [
    "print(\"#\", list(generate_next_level_candidates([(\"a\",), (\"b\",), (\"c\",)])))\n",
    "print(\"#\", list(generate_next_level_candidates([(\"a\", \"b\"), (\"a\", \"c\"), (\"c\", \"d\")])))\n",
    "print(\"#\", list(generate_next_level_candidates([(\"a\", \"b\", \"c\"), (\"a\", \"b\", \"d\"), (\"a\", \"c\", \"d\")])))\n",
    "print(\"#\", list(generate_next_level_candidates([(\"a\", \"b\", \"c\"), (\"a\", \"b\", \"d\"), (\"a\", \"c\", \"d\"), (\"b\", \"c\", \"d\")])))\n",
    "# [('a', 'b'), ('a', 'c'), ('b', 'c')]\n",
    "# []\n",
    "# []\n",
    "# [('a', 'b', 'c', 'd')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## e) Bringing general apriori together (4)\n",
    "\n",
    "Write a function `generic_apriori` that returns all patterns with length of at most (<=) `max_depth` for which `calc_value` is at least (>=) `t` on that database. Assume that the `calc_value` function is anti-monotone. Use the apriori principle to reduce the number of calls to the `calc_value` functions. The function returns a dictionary which maps all patterns for which `calc_value` exceeds as keys and maps them onto their corresponding value of `calc_value`.\n",
    "\n",
    "Assume that the database is given as a pandas DataFrame with column `'items'` which contains all the itemsets. There might be other columns which are used by the constraint. The maximum depth restricts the number of items in a pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Callable\n",
    "def generic_apriori(calc_value : Callable[[pd.DataFrame, Tuple[str]], float],\n",
    "                    t : float,\n",
    "                    items : List[Tuple[str]],\n",
    "                    max_depth : int,\n",
    "                    database : pd.DataFrame) -> Dict[Tuple[str], float]:\n",
    "    s = set()\n",
    "    d = dict()\n",
    "    for item in items:\n",
    "        s.update(getSubTuples(item, 1))\n",
    "    for item in s:\n",
    "        val = calc_value(database, item)\n",
    "        if val >= t:\n",
    "            d[item] = val\n",
    "    s = set(d.keys())\n",
    "    i = 2\n",
    "    while i <= max_depth:\n",
    "        s = set(prune_candidates(s, generate_next_level_candidates(list(s))))\n",
    "        for item in s:\n",
    "            val = calc_value(database, item)\n",
    "            if val >= t:\n",
    "                d[item] = val\n",
    "        s = set(item for item in d.keys() if len(item) == i)\n",
    "        i += 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f) Classic apriori (2)\n",
    "\n",
    "Write function `fim_apriori` that performs classic frequent-itemset mining (FIM) using the apriori principle. `min_supp` is a float between 0 and 1. It is used as a lower bound (>=) for the support. The function returns all patterns that appear at least `min_supp` times and are of at most `max_depth` depth. You should use the `generic_apriori` function.\n",
    "\n",
    "Thereby use the `calc_support` function from b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fim_apriori(database : pd.DataFrame, max_depth : int, min_supp : float) -> List[Tuple[str]]:\n",
    "    assert min_supp >=0 and  min_supp<= 1\n",
    "    d = generic_apriori(calc_support, min_supp, database['items'].values, max_depth, database)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('citron',): 0.5, ('apple',): 0.8333333333333334, ('apple', 'citron'): 0.5}\n",
      "{('apple',): 0.8333333333333334}\n",
      "{('citron',): 0.5, ('apple',): 0.8333333333333334}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# support for 1 itemsets\n",
    "# apple  : 5/6\n",
    "# banana : 2/6\n",
    "# citron : 3/6\n",
    "# milk   : 1/6\n",
    "print(fim_apriori(example_db, 2, 0.5)) # {('citron',): 0.5, ('apple',): 0.8333333333333334, ('apple', 'citron'): 0.5}\n",
    "print(fim_apriori(example_db, 1, 0.7)) # {('apple',): 0.8333333333333334}\n",
    "print(fim_apriori(example_db, 1, 0.4)) # {('citron',): 0.5, ('apple',): 0.8333333333333334}\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to identify the number of calls to min_supp\n",
    "from contextlib import contextmanager\n",
    "@contextmanager\n",
    "def my_mock(func, replace):\n",
    "    \"\"\"If a function references another function it can be replaced with this function\"\"\"\n",
    "    tmp = []\n",
    "    for key, value in replace.items():\n",
    "        current_func=func\n",
    "        splits = key.split(\".\")\n",
    "        for func_name in splits[:-1]:\n",
    "            current_func = current_func.__globals__[func_name]\n",
    "        tmp.append(current_func.__globals__[splits[-1]])\n",
    "        current_func.__globals__[splits[-1]]   = value\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        for key, value in zip(replace.keys(), tmp):\n",
    "            func.__globals__[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "{('apple', 'citron'), ('banana',), ('citron',), ('apple',), ('milk',)}\n"
     ]
    }
   ],
   "source": [
    "# the following class can be used instead of the calc_support function\n",
    "# it keeps track of the calls that have been made to it\n",
    "class MinSupport_CountCalls:\n",
    "    def __init__(self):\n",
    "        self.number_of_calls = 0\n",
    "        self.calc_support = calc_support\n",
    "        self.called_for_patterns=set()\n",
    "    def __call__(self, database, pattern):\n",
    "        self.number_of_calls +=1\n",
    "        self.called_for_patterns.add(pattern)\n",
    "        return self.calc_support(database, pattern)\n",
    "\n",
    "count_obj = MinSupport_CountCalls()\n",
    "with my_mock(fim_apriori, {'calc_support' : count_obj}):\n",
    "    fim_apriori(example_db, 2, 0.5)\n",
    "    print(count_obj.number_of_calls) # support was calculated 5 times\n",
    "    print(count_obj.called_for_patterns) # called with these arguments\n",
    "    # {('apple', 'citron'), ('banana',), ('apple',), ('milk',), ('citron',)}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "{('banana', 'milk'), ('apple', 'banana'), ('apple', 'citron'), ('banana',), ('apple', 'milk'), ('citron', 'milk'), ('citron',), ('apple',), ('milk',), ('banana', 'citron')}\n"
     ]
    }
   ],
   "source": [
    "count_obj = MinSupport_CountCalls()\n",
    "with my_mock(fim_apriori, {'calc_support' : count_obj}):\n",
    "    fim_apriori(example_db, 2, 0.3)\n",
    "    print(count_obj.number_of_calls) # support was calculated 10 times\n",
    "    print(count_obj.called_for_patterns) # called with these arguments:\n",
    "    #{('banana', 'citron'), ('apple', 'citron'), ('banana',), ('apple',), ('apple', 'banana'), ('milk',), ('banana', 'milk'), ('apple', 'milk'), ('citron',), ('citron', 'milk')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('Free Downloads',): 0.3312442678080098}\n",
      "CPU times: user 9.46 s, sys: 7.47 ms, total: 9.47 s\n",
      "Wall time: 9.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(fim_apriori(websites_db, 2, 0.3))\n",
    "#{('Free Downloads',): 0.3312442678080098}\n",
    "# Wall time: 5.29 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('Free Downloads',): 0.3312442678080098, ('Internet Explorer',): 0.2868541730357689, ('Microsoft.com Search',): 0.2587282176704372}\n",
      "CPU times: user 10.1 s, sys: 19.9 ms, total: 10.1 s\n",
      "Wall time: 10.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(fim_apriori(websites_db, 5, 0.25))\n",
    "#{('Microsoft.com Search',): 0.2587282176704372, ('Internet Explorer',): 0.2868541730357689, ('Free Downloads',): 0.3312442678080098}\n",
    "#Wall time: 5.58 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('Free Downloads',): 0.3312442678080098, ('Internet Explorer',): 0.2868541730357689, ('isapi',): 0.16294711097523693, ('Microsoft.com Search',): 0.2587282176704372, ('Free Downloads', 'Internet Explorer'): 0.16080709263222256}\n",
      "CPU times: user 10.5 s, sys: 20.1 ms, total: 10.6 s\n",
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(fim_apriori(websites_db, 5, 5260/len(websites_db)))\n",
    "# {('Internet Explorer',): 0.2868541730357689,\n",
    "# ('Microsoft.com Search',): 0.2587282176704372,\n",
    "# ('isapi',): 0.16294711097523693,\n",
    "# ('Free Downloads',): 0.3312442678080098,\n",
    "# (Free Downloads', 'Internet Explorer'): 0.16080709263222256}\n",
    "#Wall time: 5.65 s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f) Association rule miner (4)\n",
    "\n",
    "Write a function `assoc_miner` that uses `fim_apriori` and `generic_apriori` to find all association rules with at least `min_supp` and `min_conf`. It threrefore first mines all `min_supp` frequent itemsets and for each it finds the `min_conf` association rules. Use the apriori principle to mine all the association rules from a pattern.\n",
    "\n",
    "The most difficult part of this task is to find the right parameters which are passed to the `generic_apriori` function which then returns for a pattern $Z$ all association rules $A \\Rightarrow B$ with $A\\cup B = Z$ and confidence of at least (>=) `min_conf`.\n",
    "In the lecture it was presented, that the confidence of a single association rule $Z$ has a downward closure property with respect to the consequent $B$ of an association rule $A\\Rightarrow B$ with $Z=A\\cup B$. You can thus use the `generic_apriori` implementation with a custom `calc_value` function to prune the search space for association rules which come from a single itemset $Z$.\n",
    "To that extend you might want to write a function that computes the antecedent/consequence $ A = Z\\setminus B$ given the consequent/antecedent $B$.\n",
    "You might also want to use the `functools.partial` function because the `calc_value` function is different for each pattern $Z$.\n",
    "Also notice that you do not really need to query the database any more as, if $Z$ is frequent, all its subsets are also frequent i.e. the support was already calculated by the initial call to `fim_apriori`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assoc_miner(database : pd.DataFrame,\n",
    "                max_depth : int,\n",
    "                min_supp : float,\n",
    "                min_conf : float) -> pd.DataFrame:\n",
    "    tups = fim_apriori(database, max_depth, min_supp)\n",
    "    \n",
    "    return pd.DataFrame.from_records([], columns=[\"antecedent\", \"consequence\", \"support\", \"confidence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [antecedent, consequence, support, confidence]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [antecedent, consequence, support, confidence]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_rows', 300, 'display.max_columns', 7, 'display.expand_frame_repr', False):\n",
    "    print(assoc_miner(example_db, 3, 0.3, 0.01))\n",
    "#      antecedent consequence   support  confidence\n",
    "#0  (banana,)    (apple,)  0.333333         1.0\n",
    "#1   (apple,)   (banana,)  0.333333         0.4\n",
    "#2  (citron,)    (apple,)  0.500000         1.0\n",
    "#3   (apple,)   (citron,)  0.500000         0.6\n",
    "\n",
    "\n",
    "    print(assoc_miner(example_db, 4, 0.1, 0.6))\n",
    "#             antecedent consequence   support  confidence\n",
    "#0         (banana,)    (apple,)  0.333333         1.0\n",
    "#1         (citron,)    (apple,)  0.500000         1.0\n",
    "#2          (apple,)   (citron,)  0.500000         0.6\n",
    "#3  (banana, citron)    (apple,)  0.166667         1.0\n",
    "#4    (banana, milk)    (apple,)  0.166667         1.0\n",
    "#5     (apple, milk)   (banana,)  0.166667         1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedent</th>\n",
       "      <th>consequence</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [antecedent, consequence, support, confidence]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [antecedent, consequence, support, confidence]\n",
      "Index: []\n",
      "CPU times: user 18.2 s, sys: 20 ms, total: 18.2 s\n",
      "Wall time: 18.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = assoc_miner(websites_db, 4, 1000/len(websites_db), 0.6)\n",
    "display(result)\n",
    "with pd.option_context('display.max_rows', 300, 'display.max_columns', 7, 'display.expand_frame_repr', False):\n",
    "    print(str(result))\n",
    "    \n",
    "#                          antecedent               consequence   support  confidence\n",
    "#0                  (Knowledge Base,)        (Support Desktop,)  0.055212    0.608491\n",
    "#1                      (Windows 95,)  (Windows Family of OSs,)  0.032437    0.914655\n",
    "#2               (Windows95 Support,)                  (isapi,)  0.046072    0.841429\n",
    "#3     (Internet Explorer, Products )         (Free Downloads,)  0.031733    0.670110\n",
    "#4            (Knowledge Base, isapi)        (Support Desktop,)  0.033231    0.708605\n",
    "#5  (Knowledge Base, Support Desktop)                  (isapi,)  0.033231    0.601883\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Matrix factorization (7 points)\n",
    "\n",
    "In this task you should implement matrix factorization using stochastic gradient descent. The goal is to find matrices $P$ and $Q$ given a matrix $R$ such that $P \\cdot Q\\approx R$\n",
    "\n",
    "Unlike in \"normal\" matrix decomposition, the rating matrix $R$ can contain blank entries (given as 0). The factorization should be trained only on **non-blank entries**. Use squared error as your loss function and stochastic gradient descent (SGD) for optimization.\n",
    "\n",
    "\n",
    "The matrices for K latent factors are of size\n",
    "\n",
    "$$\n",
    "P: (I, K)\\\\\n",
    "Q: (K, J)\\\\\n",
    "R: (I, J)\n",
    "$$\n",
    "The Loss for one entry is $L_{i,j} = (R_{i,j} - \\sum_{k} P_{i,k} \\cdot Q_{k,j})^2$.\n",
    "The average loss for a set of index pairs is $I$\n",
    "$$\n",
    "L_I = \\frac{1}{|I|}\\sum_{(i,j) \\in I} L_{i,j}\n",
    "$$\n",
    "The partial derivatives of the loss are\n",
    "$$\n",
    "\\frac{dL_{i,j}}{dP_{i,v}} = -2(R_{i, j} - \\sum_{k} P_{i,k} \\cdot Q_{k,j})Q_{v,j} \\\\\n",
    "\\frac{dL_{i,j}}{dQ_{v,j}} = -2(R_{i, j} - \\sum_{k} P_{i,k} \\cdot Q_{k,j})P_{i,v}\n",
    "$$\n",
    "SGD update for observation $i,j$\n",
    "\n",
    "$P_{i,v} = P_{i,v} - \\alpha \\frac{dL_{i,j}}{dP_{i,v}}$\n",
    "\n",
    "$Q_{v,j} = Q_{v,j} - \\alpha \\frac{dL_{i,j}}{dQ_{v,j}}$\n",
    "\n",
    "When performing SGD, compute all the partial derivates for one index pair $i,j$ first and then update the values.\n",
    "For testing, consider the utility matrix in the file 'utility_matrix.csv'.\n",
    "\n",
    "## a) Average Loss function (1)\n",
    "Write a loss function `average_loss(R, P, Q, ijs=None)` that computes the average reconstruction loss of your matrix factorization. If `ijs` are supplied only for those indices loss is calculated, otherwise for the entire matrix. `ijs` is list of tuples of indices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_loss(R, P, Q, ijs=None):\n",
    "    if ijs is None:\n",
    "        ijs = [(i,j) for i,_ in enumerate(R) for j,_ in enumerate(R[i])]\n",
    "    L = 0\n",
    "    for i, j in ijs:\n",
    "        L += (R[i][j]-sum([P[i,k]*Q[k][j] for k,_ in enumerate(Q)]))**2\n",
    "    return L/len(ijs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14863680056176592"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example usage\n",
    "rng=default_rng(1)\n",
    "n=4\n",
    "m=5\n",
    "k=2\n",
    "P = rng.random((m,k))\n",
    "Q = rng.random((k,n))\n",
    "R = rng.random((m,n))\n",
    "average_loss(R,P,Q) \n",
    "# should be close to\n",
    "# 0.14863680056176592"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1682872357391108"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example usage\n",
    "rng=default_rng(2)\n",
    "n=10\n",
    "m=12\n",
    "k=2\n",
    "P = rng.random((m,k))\n",
    "Q = rng.random((k,n))\n",
    "R = rng.random((m,n))\n",
    "average_loss(R,P,Q)\n",
    "# should be close to\n",
    "# 0.1682872357391108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22473964744830402"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example usage\n",
    "rng=default_rng(3)\n",
    "n=10\n",
    "m=12\n",
    "k=2\n",
    "ijs = [(1,3), (4,5), (3,4)]\n",
    "P = rng.random((m,k))\n",
    "Q = rng.random((k,n))\n",
    "R = rng.random((m,n))\n",
    "average_loss(R,P,Q, ijs=ijs)\n",
    "# should be close to\n",
    "# 0.22473964744830408"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Factorize (2)\n",
    "\n",
    "Implement matrix factorization from scratch by implementing a function `matrix_factorization` with\n",
    "\n",
    "* R: Matrix of given ratings (numpy array)\n",
    "* k: Number of latent factors (int)\n",
    "* alpha: learning rate (float)\n",
    "* rng: random number generator (numpy.random.default_rng)\n",
    "* epochs: the number of epoch to train (integer)\n",
    "* ijs_train: indices to train on (list of tuples of two integers)\n",
    "* ijs_test: indices to test on (list of tuples of two integers)\n",
    "\n",
    "It returns the P, Q matrix, train-loss-list and test-loss-list. The loss is calculated after each epoch of training and is then appended to the list. One epoch is performing an update for all the `ijs_train`. Leave the `test_loss_list` empty if `ijs_test` are not specified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_factorization(R : np.array,\n",
    "                         K : int,\n",
    "                         alpha : float,\n",
    "                         rng,\n",
    "                         epochs : int,\n",
    "                         ijs_train : List[Tuple[int]],\n",
    "                         ijs_test : List[Tuple[int]]=None):\n",
    "    #randomly initialize matrices\n",
    "    U,I = R.shape\n",
    "    P = rng.random((U,K))\n",
    "    Q = rng.random((K,I))\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for _ in range(epochs):\n",
    "        for i,j in ijs_train:\n",
    "            grad_P = [-2*(R[i][j]-sum([P[i,k]*Q[k][j] for k,_ in enumerate(Q)]))*Q[v][j] for v,_ in enumerate(Q)]\n",
    "            grad_Q = [-2*(R[i][j]-sum([P[i,k]*Q[k][j] for k,_ in enumerate(Q)]))*P[i][v] for v,_ in enumerate(P[i])]\n",
    "            for v,_ in enumerate(Q):\n",
    "                P[i][v] = P[i][v] - alpha*grad_P[v]\n",
    "                Q[v][j] = Q[v][j] - alpha*grad_Q[v]\n",
    "        train_losses.append(average_loss(R,P,Q, ijs_train))\n",
    "        if ijs_test is not None:\n",
    "            test_losses.append(average_loss(R,P,Q, ijs_test))\n",
    "    return P, Q, train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.12028559684065, 5.702120509768838]\n"
     ]
    }
   ],
   "source": [
    "# Generate example:\n",
    "R = np.genfromtxt('utility_matrix.csv', delimiter=',',dtype=np.float64)\n",
    "rng=default_rng(1)\n",
    "ijs = [(i,j) for i, j in zip(*np.nonzero(R))]\n",
    "epochs=2\n",
    "P, Q, l_train,l_test = matrix_factorization(R, 4, 0.001, rng, epochs, ijs)\n",
    "print(l_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# epochs = 1\n",
    "# P\n",
    "array([[0.53030017, 0.96568957, 0.17039495, 0.96478729],\n",
    "       [0.32009701, 0.43594586, 0.83751995, 0.41845338],\n",
    "       [0.56471082, 0.03854926, 0.76744862, 0.54845707],\n",
    "       [0.35681508, 0.80943855, 0.33222907, 0.47213126],\n",
    "       [0.14975727, 0.41927103, 0.22232818, 0.2733064 ],\n",
    "       [0.7632887 , 0.29660883, 0.50014336, 0.99427529],\n",
    "       [0.97469191, 0.74041843, 0.55915683, 0.29096398],\n",
    "       [0.17618862, 0.98158305, 0.53054081, 0.12794982],\n",
    "       [0.62839181, 0.78786586, 0.61887645, 0.92215325],\n",
    "       [0.05183058, 0.54145109, 0.47416302, 0.07551933],\n",
    "       [0.65114207, 0.86260615, 0.60851652, 0.27028487],\n",
    "       [0.8600531 , 0.52663855, 0.53355849, 0.76868257],\n",
    "       [0.15773973, 0.82380165, 0.69349028, 0.79731367],\n",
    "       [0.20945627, 0.82255223, 0.21504987, 0.09967779],\n",
    "       [0.86684443, 0.87304182, 0.89573504, 0.48592394],\n",
    "       [0.29209354, 0.0288247 , 0.67087993, 0.73952057],\n",
    "       [0.8405461 , 0.28475842, 0.22651716, 0.64898595],\n",
    "       [0.81803697, 0.97569678, 0.1701876 , 0.49792483],\n",
    "       [0.90125284, 0.42775465, 0.59711584, 0.03434974],\n",
    "       [0.68053857, 0.93020444, 0.83722127, 0.89347768]])\n",
    "# Q.T\n",
    "array([[0.67905308, 0.71703279, 0.6783386 , 0.28217364],\n",
    "       [0.28671191, 0.2245397 , 0.46818009, 0.11601512],\n",
    "       [0.79664329, 0.43092275, 0.8926013 , 0.28903537],\n",
    "       [0.24819143, 0.041076  , 0.66335624, 0.79366653],\n",
    "       [0.85183337, 0.29365763, 0.83269148, 0.71732725],\n",
    "       [0.10102076, 0.46937421, 0.37185553, 0.15782279],\n",
    "       [0.85353666, 0.1410099 , 0.56891687, 0.40167918],\n",
    "       [0.20774953, 0.67103893, 0.24104121, 0.46453524],\n",
    "       [0.39950197, 0.41217316, 1.01234667, 0.68805916],\n",
    "       [0.36029593, 0.78600697, 0.28899546, 0.50123434]])\n",
    "# l_train\n",
    " [6.120285596840649]\n",
    "# l_test\n",
    " []\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# epochs = 2\n",
    "# P\n",
    "array([[0.54898348, 0.98157321, 0.19648697, 0.98124088],\n",
    "       [0.32886539, 0.44907869, 0.84768277, 0.42812938],\n",
    "       [0.58012071, 0.05003125, 0.78152081, 0.55921641],\n",
    "       [0.38441392, 0.83141728, 0.3615508 , 0.49147467],\n",
    "       [0.1662158 , 0.43662063, 0.24186757, 0.2852157 ],\n",
    "       [0.77648636, 0.31330686, 0.51525653, 1.00812756],\n",
    "       [0.98826968, 0.75669342, 0.57721966, 0.30538718],\n",
    "       [0.19191371, 0.99378748, 0.54508121, 0.14019524],\n",
    "       [0.63329215, 0.79907542, 0.62449599, 0.92703749],\n",
    "       [0.0645117 , 0.55504459, 0.48935008, 0.08918713],\n",
    "       [0.66116901, 0.8730181 , 0.62392472, 0.28066017],\n",
    "       [0.88065328, 0.54456397, 0.55651066, 0.78492335],\n",
    "       [0.16776704, 0.82830911, 0.70377071, 0.80760509],\n",
    "       [0.22800606, 0.84385433, 0.23923373, 0.11851386],\n",
    "       [0.87842173, 0.88485997, 0.91440644, 0.49974584],\n",
    "       [0.31099925, 0.0515082 , 0.69662905, 0.75999372],\n",
    "       [0.84561165, 0.2879153 , 0.23761533, 0.65868291],\n",
    "       [0.83158875, 0.98837475, 0.19004603, 0.51386302],\n",
    "       [0.90773606, 0.43283726, 0.604502  , 0.04412471],\n",
    "       [0.6871331 , 0.94118386, 0.84689142, 0.90100045]])\n",
    "# Q.T\n",
    "array([[0.69701391, 0.74199718, 0.70229101, 0.30657812],\n",
    "       [0.32711821, 0.26978508, 0.50493145, 0.15823725],\n",
    "       [0.82398116, 0.46445741, 0.91741003, 0.31943684],\n",
    "       [0.28343279, 0.07483107, 0.69377651, 0.82312758],\n",
    "       [0.87141708, 0.32355552, 0.8543836 , 0.73583079],\n",
    "       [0.13855796, 0.51664766, 0.40165922, 0.18632909],\n",
    "       [0.88099643, 0.17496533, 0.59388225, 0.42629548],\n",
    "       [0.24995286, 0.70774974, 0.28510039, 0.50711432],\n",
    "       [0.42283823, 0.44255497, 1.02792469, 0.71036147],\n",
    "       [0.40242657, 0.84484156, 0.33390436, 0.54527132]])\n",
    "# l_train\n",
    "[6.120285596840649, 5.702120509768838]\n",
    "# l_test\n",
    "[]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Evaluate the training (1+1+2)\n",
    "\n",
    "Evaluate the training by visualising the training- and test-loss as a function of epochs for different values of `alpha`. Apply it to the 'utility_matrix.csv'.  Perform the train test split like so (it's roughly 80/20):\n",
    "\n",
    "```python\n",
    "R = np.genfromtxt('utility_matrix.csv', delimiter=',',dtype=np.float)\n",
    "rng = default_rng(13)\n",
    "ijs = [(i,j) for i, j in zip(*np.nonzero(R))]\n",
    "rng.shuffle(ijs)\n",
    "ijs_train=ijs[:122]\n",
    "ijs_test=ijs[122:]\n",
    "```\n",
    "\n",
    "The different values for `alpha` are `[0.05, 0.01, 0.001, 0.0001]`.\n",
    "Use K=5 latent factors. Train for 20 and 500 epochs.\n",
    "Save your plot as `training20.png` and `training500.png`.\n",
    "Save a description + explanation of what is happening as `training.txt`. Write 4+ sentences.\n",
    "\n",
    "You will get 1 point for the plots each and two points for the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_dict = {}\n",
    "R = np.genfromtxt('utility_matrix.csv', delimiter=',',dtype=np.float)\n",
    "rng=default_rng(13)\n",
    "ijs = [(i,j) for i, j in zip(*np.nonzero(R))]\n",
    "rng.shuffle(ijs)\n",
    "ijs_train=ijs[:122]\n",
    "ijs_test=ijs[122:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_loss = plt.figure(figsize=(12, 12))\n",
    "for a in [0.05, 0.01, 0.001, 0.0001]:\n",
    "    P, Q, l_train,l_test = matrix_factorization(R, 5, a, rng, 20, ijs_train, ijs_test)\n",
    "    plt.plot(l_train, label=f\"Train Loss a ={a}\")\n",
    "    plt.plot(l_test, label=f\"Test Loss a ={a}\")\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.xticks(np.arange(len(l_test)), list(map(lambda x: str(x+1), np.arange(len(l_test)))))\n",
    "plt.legend()\n",
    "plt.savefig(\"training20.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_loss = plt.figure(figsize=(32, 32))\n",
    "for a in [0.05, 0.01, 0.001, 0.0001]:\n",
    "    P, Q, l_train,l_test = matrix_factorization(R, 5, a, rng, 500, ijs_train, ijs_test)\n",
    "    plt.plot(l_train, label=f\"Train Loss a ={a}\")\n",
    "    plt.plot(l_test, label=f\"Test Loss a ={a}\")\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.xticks(np.arange(len(l_test)), list(map(lambda x: str(x+1) if (x+1) % 10 == 0 else \"\", np.arange(len(l_test)))))\n",
    "plt.legend()\n",
    "plt.savefig(\"training500.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training.txt\", \"w\") as f:\n",
    "    f.write(\"\"\"We are observing that we have to adjust the learning rate not too big and not to small. With big learning rates we overfitt very quickly and never reach a minimum - the loss is going up and diverging then. \n",
    "With learning rates too small we have a convergance but a very slow and steady one and we seam also not to get close to an optimum. The best approach seems to be taking a learning rate of 0.001 and utilizing the early stop!\n",
    "This would mean to finsih training here after about 30 epochs. While we have several nice converging training losses this one is the only one where also the test loss is accurate.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
